{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "<hr>\n",
    "\n",
    "***Automatic Text Summarization*** is the process of shortening a text document using the deep learning methods. More specifically we will use a seq2seq model for this purpose. This model is widely used in industry today. Search engines are an example. Others include summarization of documents, image collections and videos. \n",
    "\n",
    "<img width=\"600px\" src=\"assets/intro.png\">\n",
    "<br><br>\n",
    "In this Jupyter Notebook, we will go through the following chapters:\n",
    "\n",
    "- **Chapter 1:** Data Prepration\n",
    "- **Chapter 2:** Data Preprocessing & Embedding\n",
    "- **Chapter 3:** Training the Seq2Seq Model\n",
    "- **Chapter 4:** Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "# Chapter 1: Data Prepration\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this chapter, we will make our data ready. Furthermore, we make sure the necessary libraries have been installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 1.1: DOWNLOAD AND IMPORT PACKAGES\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now, let's import all the packages and libraries that we are going to use throughout this jupyter notebook. Also make sure that you have installed Tensorflow version 1.12 otherwise the codes won't run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.6/site-packages (4.28.1)\n",
      "Requirement already satisfied: pandas in /anaconda3/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.6/site-packages (from pandas) (2018.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /anaconda3/lib/python3.6/site-packages (from pandas) (1.16.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /anaconda3/lib/python3.6/site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (1.16.3)\n",
      "Requirement already satisfied: contractions in /anaconda3/lib/python3.6/site-packages (0.0.18)\n",
      "Requirement already satisfied: tensorflow in /anaconda3/lib/python3.6/site-packages (1.12.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.32.3)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.16.3)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.0.7)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.12.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.0.9)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: tensorflow_hub in /anaconda3/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (1.16.3)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (3.9.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow_hub) (41.0.1)\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.6/site-packages (3.4)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /anaconda3/lib/python3.6/site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install contractions\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_hub\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import time\n",
    "import re\n",
    "import html\n",
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import contractions\n",
    "\n",
    "# Import the deep learning libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Import the NLP libraries\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/soheilmohammadpour/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download punkt sentence tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  1.12.0\n"
     ]
    }
   ],
   "source": [
    "# Check the tensorflow version\n",
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "assert tf.__version__ == \"1.12.0\", print(\"Please Install Tensorflow Version 1.12.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 1.2: DOWNLOAD & LOAD THE DATASET\n",
    "\n",
    "<hr>\n",
    "\n",
    "The dataset that we are going to use is called \"Amazon Find Food Reviews\". This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories. Our aim is to input a review (Text column) and automatically create a summary (Summary colum) for it. \n",
    "\n",
    "<img width=\"400px\" src=\"assets/amazon_food.png\">\n",
    "\n",
    "You can download the dataset from the following link:\n",
    "\n",
    "https://www.kaggle.com/snap/amazon-fine-food-reviews/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"./dataset/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at first 5 rows of dataset\n",
    "data.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:  (568454, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 1.3: EXPLORE THE DATASET\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this part, we will make our dataset to be ready for data preprocessing. More specifically, we will remove the missing values, we will get the relevant columns and ignore the rest, and lastly we will remove the texts if it's too large or if it's too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: \n",
      "\n",
      " Id                         0\n",
      "ProductId                  0\n",
      "UserId                     0\n",
      "ProfileName               16\n",
      "HelpfulnessNumerator       0\n",
      "HelpfulnessDenominator     0\n",
      "Score                      0\n",
      "Time                       0\n",
      "Summary                   27\n",
      "Text                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the total missing values\n",
    "print(\"Missing Values: \\n\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing values in \"Summary\" column\n",
    "data.dropna(axis = 0, how = 'any', subset = [\"Summary\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the columns \"Summary\" and \"Text\"\n",
    "data = data[['Summary', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the dataset\n",
    "data.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the text with length between 100 and 150\n",
    "\n",
    "# Initialize the empty lists\n",
    "texts = []\n",
    "summaries = []\n",
    "\n",
    "# Iterate through text and its summary\n",
    "for i_text, i_summary in zip(data[\"Text\"], data[\"Summary\"]):\n",
    "    \n",
    "    # If length is between 100 and 150\n",
    "    if 100 < len(i_text) < 150:\n",
    "        \n",
    "        # Append the text and summary into list\n",
    "        texts.append(i_text)\n",
    "        summaries.append(i_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Texts Lists:  78862\n",
      "Length of Summaries Lists:  78862\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Texts Lists: \", len(texts))\n",
    "print(\"Length of Summaries Lists: \", len(summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "Summary:  Great taffy\n",
      "\n",
      "\n",
      "Text:  This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!\n",
      "Summary:  Wonderful, tasty taffy\n",
      "\n",
      "\n",
      "Text:  Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too\n",
      "Summary:  Yay Barley\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first 3 texts and its summary\n",
    "for i_text, i_summary in zip(texts[:3], summaries[:3]):\n",
    "    print(\"Text: \", i_text)\n",
    "    print(\"Summary: \", i_summary)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Data Preprocessing & Embedding\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this chapter we will apply all the necessary preprocessing steps into our text. Then we will create embedding for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 2.1: CLEAN AND PREPARE THE DATASET\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this lessson, we will write down some function for creating mini-batches and sequence padding. Afterward we will apply the text preprocessing steps for making our dataset ready for neural network. More specifically, we will apply the following steps:\n",
    "1. Lowercase the text\n",
    "2. Fix the contractions (e.g. she's -> she is, he's -> he is, etc.)\n",
    "3. Remove the punctuations\n",
    "4. Remove some specific characters inside the given dataset\n",
    "5. Remove the extra spaces\n",
    "6. Tokenizing the texts into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(inputs, targets, minibatch_size):\n",
    "    \"\"\"batch generator. yields x and y batch.\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    for inp, tgt in zip(inputs, targets):\n",
    "        if len(x_batch) == minibatch_size and len(y_batch) == minibatch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "        x_batch.append(inp)\n",
    "        y_batch.append(tgt)\n",
    "\n",
    "    if len(x_batch) != 0:\n",
    "        for inp, tgt in zip(inputs, targets):\n",
    "            if len(x_batch) != minibatch_size:\n",
    "                x_batch.append(inp)\n",
    "                y_batch.append(tgt)\n",
    "            else:\n",
    "                break\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, pad_tok, tail=True):\n",
    "    \"\"\"Pads the sentences, so that all sentences in a batch have the same length.\n",
    "    \"\"\"\n",
    "\n",
    "    max_length = max(len(x) for x in sequences)\n",
    "\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        if tail:\n",
    "            seq_ = seq[:max_length] + [pad_tok] * max(max_length - len(seq), 0)\n",
    "        else:\n",
    "            seq_ = [pad_tok] * max(max_length - len(seq), 0) + seq[:max_length]\n",
    "\n",
    "        sequence_padded += [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=97):\n",
    "    \"\"\"helper function to reset the default graph. this often\n",
    "       comes handy when using jupyter noteboooks.\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_results(preds, ind2word, word2ind, converted_summaries, converted_texts, use_bleu=False):\n",
    "    \"\"\"Plots the actual text and summary and the corresponding created summary.\n",
    "    takes care of whether beam search or greedy decoder was used.\n",
    "    \"\"\"\n",
    "    beam = False\n",
    "\n",
    "    if len(np.array(preds).shape) == 4:\n",
    "        beam = True\n",
    "\n",
    "    '''Bleu score is not used correctly here, but serves as reference.\n",
    "    '''\n",
    "    if use_bleu:\n",
    "        bleu_scores = []\n",
    "\n",
    "    for pred, summary, text, seq_length in zip(preds[0],\n",
    "                                               converted_summaries,\n",
    "                                               converted_texts,\n",
    "                                               [len(inds) for inds in converted_summaries]):\n",
    "        print('\\n\\n\\n', 100 * '-')\n",
    "        if beam:\n",
    "            actual_text = [ind2word[word] for word in text if\n",
    "                           word != word2ind[\"<SOS>\"] and word != word2ind[\"<EOS>\"]]\n",
    "            actual_summary = [ind2word[word] for word in summary if\n",
    "                              word != word2ind['<EOS>'] and word != word2ind['<SOS>']]\n",
    "\n",
    "            created_summary = []\n",
    "            for word in pred:\n",
    "                if word[0] != word2ind['<SOS>'] and word[0] != word2ind['<EOS>']:\n",
    "                    created_summary.append(ind2word[word[0]])\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            print('Actual Text:\\n{}\\n'.format(' '.join(actual_text)))\n",
    "            print('Actual Summary:\\n{}\\n'.format(' '.join(actual_summary)))\n",
    "            print('Created Summary:\\n{}\\n'.format(' '.join(created_summary)))\n",
    "            if use_bleu:\n",
    "                bleu_score = sentence_bleu([actual_summary], created_summary)\n",
    "                bleu_scores.append(bleu_score)\n",
    "                print('Bleu-score:', bleu_score)\n",
    "\n",
    "            print()\n",
    "\n",
    "\n",
    "        else:\n",
    "            actual_text = [ind2word[word] for word in text if\n",
    "                           word != word2ind[\"<SOS>\"] and word != word2ind[\"<EOS>\"]]\n",
    "            actual_summary = [ind2word[word] for word in summary if\n",
    "                              word != word2ind['<EOS>'] and word != word2ind['<SOS>']]\n",
    "            created_summary = [ind2word[word] for word in pred if\n",
    "                               word != word2ind['<EOS>'] and word != word2ind['<SOS>']]\n",
    "\n",
    "            print('Actual Text:\\n{}\\n'.format(' '.join(actual_text)))\n",
    "            print('Actual Summary:\\n{}\\n'.format(' '.join(actual_summary)))\n",
    "            print('Created Summary:\\n{}\\n'.format(' '.join(created_summary)))\n",
    "            if use_bleu:\n",
    "                bleu_score = sentence_bleu([actual_summary], created_summary)\n",
    "                bleu_scores.append(bleu_score)\n",
    "                print('Bleu-score:', bleu_score)\n",
    "\n",
    "    if use_bleu:\n",
    "        bleu_score = np.mean(bleu_scores)\n",
    "        print('\\n\\n\\nTotal Bleu Score:', bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(text, keep_most=False):\n",
    "    \"\"\"\n",
    "    Helper function to remove html, unneccessary spaces and punctuation.\n",
    "    Args:\n",
    "        text: String.\n",
    "        keep_most: Boolean. depending if True or False, we either\n",
    "                   keep only letters and numbers or also other characters.\n",
    "    Returns:\n",
    "        processed text.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = fixup(text)\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    if keep_most:\n",
    "        text = re.sub(r\"[^a-z0-9%!?.,:()/]\", \" \", text)\n",
    "    else:\n",
    "        text = re.sub(r\"[^a-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"    \", \" \", text)\n",
    "    text = re.sub(r\"   \", \" \", text)\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixup(x):\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, keep_most=False):\n",
    "    \"\"\"\n",
    "    Splits the text into sentences, preprocesses\n",
    "       and tokenizes each sentence.\n",
    "    Args:\n",
    "        text: String. multiple sentences.\n",
    "        keep_most: Boolean. depending if True or False, we either\n",
    "                   keep only letters and numbers or also other characters.\n",
    "    Returns:\n",
    "        preprocessed and tokenized text.\n",
    "    \"\"\"\n",
    "    tokenized = []\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        sentence = preprocess_sentence(sentence, keep_most)\n",
    "        sentence = nltk.word_tokenize(sentence)\n",
    "        for token in sentence:\n",
    "            tokenized.append(token)\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts_and_summaries(texts,\n",
    "                                   summaries,\n",
    "                                   keep_most=False):\n",
    "    \"\"\"iterates given list of texts and given list of summaries and tokenizes every\n",
    "       review using the tokenize_review() function.\n",
    "       apart from that we count up all the words in the texts and summaries.\n",
    "       returns: - processed texts\n",
    "                - processed summaries\n",
    "                - array containing all the unique words together with their counts\n",
    "                  sorted by counts.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    processed_texts = []\n",
    "    processed_summaries = []\n",
    "    words = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = preprocess(text, keep_most)\n",
    "        for word in text:\n",
    "            words.append(word)\n",
    "        processed_texts.append(text)\n",
    "    for summary in summaries:\n",
    "        summary = preprocess(summary, keep_most)\n",
    "        for word in summary:\n",
    "            words.append(word)\n",
    "\n",
    "        processed_summaries.append(summary)\n",
    "    words_counted = Counter(words).most_common()\n",
    "    print('Processing Time: ', time.time() - start_time)\n",
    "\n",
    "    return processed_texts, processed_summaries, words_counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function gives us the option to keep_most of the characters inisde the texts and summaries, meaning\n",
    "# punctuation, question marks, slashes...\n",
    "# or we can set it to False, meaning we only want to keep letters and numbers like here.\n",
    "processed_texts, processed_summaries, words_counted = preprocess_texts_and_summaries(\n",
    "    texts,\n",
    "    summaries,\n",
    "    keep_most=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t,s in zip(processed_texts[:5], processed_summaries[:5]):\n",
    "    print('Text\\n:', t, '\\n')\n",
    "    print('Summary:\\n', s, '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 2.2: CREATE LOOKUP DICTIONARIES\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now it's time to create two dictionaries; One that converts text into integers (word2int) and another one that converts integers into text (int2word).\n",
    "\n",
    "<img width=\"800px\" src=\"assets/int2word.jpeg\">\n",
    "<p style=\"font-size:9px; text-align:right; color:gray;\" >Image Taken From hackernoon.com</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_inds_dicts(words_counted,\n",
    "                           specials=None,\n",
    "                           min_occurences=0):\n",
    "    \"\"\" creates lookup dicts from word to index and back.\n",
    "        returns the lookup dicts and an array of words that were not used,\n",
    "        due to rare occurence.\n",
    "    \"\"\"\n",
    "    missing_words = []\n",
    "    word2ind = {}\n",
    "    ind2word = {}\n",
    "    i = 0\n",
    "\n",
    "    if specials is not None:\n",
    "        for sp in specials:\n",
    "            word2ind[sp] = i\n",
    "            ind2word[i] = sp\n",
    "            i += 1\n",
    "\n",
    "    for (word, count) in words_counted:\n",
    "        if count >= min_occurences:\n",
    "            word2ind[word] = i\n",
    "            ind2word[i] = word\n",
    "            i += 1\n",
    "        else:\n",
    "            missing_words.append(word)\n",
    "\n",
    "    return word2ind, ind2word, missing_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(review, word2ind):\n",
    "    \"\"\" converts the given sent to int values corresponding to the given word2ind\"\"\"\n",
    "    inds = []\n",
    "    unknown_words = []\n",
    "\n",
    "    for word in review:\n",
    "        if word in word2ind.keys():\n",
    "            inds.append(int(word2ind[word]))\n",
    "        else:\n",
    "            inds.append(int(word2ind['<UNK>']))\n",
    "            unknown_words.append(word)\n",
    "\n",
    "    return inds, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_inds(input, word2ind, eos=False, sos=False):\n",
    "    converted_input = []\n",
    "    all_unknown_words = set()\n",
    "\n",
    "    for inp in input:\n",
    "        converted_inp, unknown_words = convert_sentence(inp, word2ind)\n",
    "        if eos:\n",
    "            converted_inp.append(word2ind['<EOS>'])\n",
    "        if sos:\n",
    "            converted_inp.insert(0, word2ind['<SOS>'])\n",
    "        converted_input.append(converted_inp)\n",
    "        all_unknown_words.update(unknown_words)\n",
    "\n",
    "    return converted_input, all_unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_inds_to_text(inds, ind2word, preprocess=False):\n",
    "    \"\"\" convert the given indexes back to text \"\"\"\n",
    "    words = [ind2word[word] for word in inds]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = [\"<EOS>\", \"<SOS>\",\"<PAD>\",\"<UNK>\"]\n",
    "word2ind, ind2word,  missing_words = create_word_inds_dicts(words_counted,\n",
    "                                                                       specials = specials)\n",
    "print(len(word2ind), len(ind2word), len(missing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts words in texts and summaries to indices\n",
    "# it looks like we have to set eos here to False\n",
    "converted_texts, unknown_words_in_texts = convert_to_inds(processed_texts,\n",
    "                                                                                word2ind,\n",
    "                                                                                eos = False)\n",
    "\n",
    "converted_summaries, unknown_words_in_summaries = convert_to_inds(processed_summaries,\n",
    "                                                                                        word2ind,\n",
    "                                                                                        eos = True,\n",
    "                                                                                        sos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Integer text: \\n\", converted_texts[0], \"\\n\")\n",
    "print(\"Integer summary: \\n\", converted_summaries[0], \"\\n\")\n",
    "print(\"Converted Text: \\n\", convert_inds_to_text(converted_texts[0], ind2word), \"\\n\")\n",
    "print(\"Converted Summary: \\n\", convert_inds_to_text(converted_summaries[0], ind2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 2.3: EMBEDDING  \n",
    "\n",
    "<hr>\n",
    "\n",
    "Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. Word embeddings are distributed representations of text in an n-dimensional space. These are essential for solving most NLP problems. Optionally we can use pretrained word embeddings. Those have proved to increase training speed and accuracy. Here we can use two different options: glove embedding and tf_hub embedding. However the ones from tf_hub worked better.\n",
    "\n",
    "<img width=\"300px\" src=\"assets/embeddings.png\">\n",
    "<p style=\"font-size:9px; text-align:right; color:gray;\" >Image Taken From udacity.com</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embeddings(path):\n",
    "    \"\"\"loads pretrained embeddings. stores each embedding in a\n",
    "       dictionary with its corresponding word\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding_vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = embedding_vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_embedding_matrix(word2ind,\n",
    "                                     pretrained_embeddings_path,\n",
    "                                     save_path,\n",
    "                                     embedding_dim=300):\n",
    "    \"\"\"creates embedding matrix for each word in word2ind. if that words is in\n",
    "       pretrained_embeddings, that vector is used. otherwise initialized\n",
    "       randomly.\n",
    "    \"\"\"\n",
    "    pretrained_embeddings = load_pretrained_embeddings(pretrained_embeddings_path)\n",
    "    embedding_matrix = np.zeros((len(word2ind), embedding_dim), dtype=np.float32)\n",
    "    for word, i in word2ind.items():\n",
    "        if word in pretrained_embeddings.keys():\n",
    "            embedding_matrix[i] = pretrained_embeddings[word]\n",
    "        else:\n",
    "            embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            embedding_matrix[i] = embedding\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    np.save(save_path, embedding_matrix)\n",
    "    return np.array(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to GLOVE embedding\n",
    "#glove_embeddings_path = './glove/glove.6B.300d.txt'\n",
    "\n",
    "# Path to save the embedding matrix\n",
    "#embedding_matrix_save_path = './embeddings/embedding.npy'\n",
    "\n",
    "# Create an embedding matrix for each word in word2ind\n",
    "#emb = create_and_save_embedding_matrix(word2ind, glove_embeddings_path, embedding_matrix_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0804 16:06:59.448579 4386010560 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings from tf_hub.\n",
    "embed = hub.Module(\"https://tfhub.dev/google/Wiki-words-250/1\")      # embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "emb = embed([key for key in word2ind.keys()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Initialize all tables of the default graph\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    # Run the session\n",
    "    embedding = sess.run(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Size:  (25031, 250)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding Size: \", embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embedding\n",
    "np.save('embeddings/embedding.npy', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Chapter 3: Training the Seq2Seq Model\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this chapter, we will start training our seq2seq model for text summarization. We will begin with setting our hyperparameters, then we will initialize our model and afterward train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 3.1: SET HYPERPARAMETERS & PATHS\n",
    "\n",
    "<hr>\n",
    "\n",
    "Before initializing the seq2seq model, let's set our hyperparameters. We will come back to this part more often in order to change the hyperparameters. This is a important part because it determines the big part of our accuracy. So take your time for changing the value and monitoring the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparametes\n",
    "num_layers_encoder = 4\n",
    "num_layers_decoder = 4\n",
    "rnn_size_encoder = 512\n",
    "rnn_size_decoder = 512\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 1\n",
    "clip = 5\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.0005\n",
    "max_lr=0.005\n",
    "learning_rate_decay_steps = 700\n",
    "learning_rate_decay = 0.90\n",
    "\n",
    "\n",
    "pretrained_embeddings_path = './embeddings/embedding.npy'\n",
    "summary_dir = os.path.join('./tensorboard', str('Nn_' + str(rnn_size_encoder) + '_Lr_' + str(learning_rate)))\n",
    "\n",
    "\n",
    "use_cyclic_lr = True\n",
    "inference_targets=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 3.2: IMPLEMENT THE SEQ2SEQ MODEL\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now let's implement the seq2seq model. Every seq2seq model is divided into two parts: encoder and decoder. The encoder encodes the input sequence into a fixed-length context vector. This vector is an internal representation of the text. This context vector is then decoded into the output sequence by the decoder. \n",
    "\n",
    "<img src=\"assets/encoder-decoder.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "#import summarizer_model_utils\n",
    "\n",
    "\n",
    "class Summarizer:\n",
    "\n",
    "    def __init__(self, word2ind, ind2word, save_path, mode='TRAIN', num_layers_encoder=1, num_layers_decoder=1,\n",
    "                 embedding_dim=300, rnn_size_encoder=256, rnn_size_decoder=256, learning_rate=0.001,\n",
    "                 learning_rate_decay=0.9, learning_rate_decay_steps=100, max_lr=0.01, keep_probability=0.8,\n",
    "                 batch_size=64, beam_width=10, epochs=20, eos=\"<EOS>\", sos=\"<SOS>\", pad='<PAD>', clip=5,\n",
    "                 inference_targets=False, pretrained_embeddings_path=None, summary_dir=None, use_cyclic_lr=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word2ind: lookup dict from word to index.\n",
    "            ind2word: lookup dict from index to word.\n",
    "            save_path: path to save the tf model to in the end.\n",
    "            mode: String. 'TRAIN' or 'INFER'. depending on which mode we use\n",
    "                  a different graph is created.\n",
    "            num_layers_encoder: Float. Number of encoder layers. defaults to 1.\n",
    "            num_layers_decoder: Float. Number of decoder layers. defaults to 1.\n",
    "            embedding_dim: dimension of the embedding vectors in the embedding matrix.\n",
    "                           every word has a embedding_dim 'long' vector.\n",
    "            rnn_size_encoder: Integer. number of hidden units in encoder. defaults to 256.\n",
    "            rnn_size_decoder: Integer. number of hidden units in decoder. defaults to 256.\n",
    "            learning_rate: Float.\n",
    "            learning_rate_decay: only if exponential learning rate is used.\n",
    "            learning_rate_decay_steps: Integer.\n",
    "            max_lr: only used if cyclic learning rate is used.\n",
    "            keep_probability: Float.\n",
    "            batch_size: Integer. Size of minibatches.\n",
    "            beam_width: Integer. Only used in inference, for Beam Search.('INFER'-mode)\n",
    "            epochs: Integer. Number of times the training is conducted\n",
    "                    on the whole training data.\n",
    "            eos: EndOfSentence tag.\n",
    "            sos: StartOfSentence tag.\n",
    "            pad: Padding tag.\n",
    "            clip: Value to clip the gradients to in training process.\n",
    "            inference_targets:\n",
    "            pretrained_embeddings_path: Path to pretrained embeddings. Has to be .npy\n",
    "            summary_dir: Directory the summaries are written to for tensorboard.\n",
    "            use_cyclic_lr: Boolean.\n",
    "        \"\"\"\n",
    "\n",
    "        self.word2ind = word2ind\n",
    "        self.ind2word = ind2word\n",
    "        self.vocab_size = len(word2ind)\n",
    "        self.num_layers_encoder = num_layers_encoder\n",
    "        self.num_layers_decoder = num_layers_decoder\n",
    "        self.rnn_size_encoder = rnn_size_encoder\n",
    "        self.rnn_size_decoder = rnn_size_decoder\n",
    "        self.save_path = save_path\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mode = mode.upper()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.learning_rate_decay_steps = learning_rate_decay_steps\n",
    "        self.keep_probability = keep_probability\n",
    "        self.batch_size = batch_size\n",
    "        self.beam_width = beam_width\n",
    "        self.eos = eos\n",
    "        self.sos = sos\n",
    "        self.clip = clip\n",
    "        self.pad = pad\n",
    "        self.epochs = epochs\n",
    "        self.inference_targets = inference_targets\n",
    "        self.pretrained_embeddings_path = pretrained_embeddings_path\n",
    "        self.use_cyclic_lr = use_cyclic_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.summary_dir = summary_dir\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.add_placeholders()\n",
    "        self.add_embeddings()\n",
    "        self.add_lookup_ops()\n",
    "        self.initialize_session()\n",
    "        self.add_seq2seq()\n",
    "        self.saver = tf.train.Saver()\n",
    "        print('Graph built.')\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.ids_1 = tf.placeholder(tf.int32,\n",
    "                                    shape=[None, None],\n",
    "                                    name='ids_source')\n",
    "        self.ids_2 = tf.placeholder(tf.int32,\n",
    "                                    shape=[None, None],\n",
    "                                    name='ids_target')\n",
    "        self.sequence_lengths_1 = tf.placeholder(tf.int32,\n",
    "                                                 shape=[None],\n",
    "                                                 name='sequence_length_source')\n",
    "        self.sequence_lengths_2 = tf.placeholder(tf.int32,\n",
    "                                                 shape=[None],\n",
    "                                                 name='sequence_length_target')\n",
    "        self.maximum_iterations = tf.reduce_max(self.sequence_lengths_2,\n",
    "                                                name='max_dec_len')\n",
    "\n",
    "    def create_word_embedding(self, embed_name, vocab_size, embed_dim):\n",
    "        \"\"\"Creates embedding matrix in given shape - [vocab_size, embed_dim].\n",
    "        \"\"\"\n",
    "        embedding = tf.get_variable(embed_name,\n",
    "                                    shape=[vocab_size, embed_dim],\n",
    "                                    dtype=tf.float32)\n",
    "        return embedding\n",
    "\n",
    "    def add_embeddings(self):\n",
    "        \"\"\"Creates the embedding matrix. In case path to pretrained embeddings is given,\n",
    "           that embedding is loaded. Otherwise created.\n",
    "        \"\"\"\n",
    "        if self.pretrained_embeddings_path is not None:\n",
    "            self.embedding = tf.Variable(np.load(self.pretrained_embeddings_path),\n",
    "                                         name='embedding')\n",
    "            print('Loaded pretrained embeddings.')\n",
    "        else:\n",
    "            self.embedding = self.create_word_embedding('embedding',\n",
    "                                                        self.vocab_size,\n",
    "                                                        self.embedding_dim)\n",
    "\n",
    "    def add_lookup_ops(self):\n",
    "        \"\"\"Additional lookup operation for both source embedding and target embedding matrix.\n",
    "        \"\"\"\n",
    "        self.word_embeddings_1 = tf.nn.embedding_lookup(self.embedding,\n",
    "                                                        self.ids_1,\n",
    "                                                        name='word_embeddings_1')\n",
    "        self.word_embeddings_2 = tf.nn.embedding_lookup(self.embedding,\n",
    "                                                        self.ids_2,\n",
    "                                                        name='word_embeddings_2')\n",
    "\n",
    "    def make_rnn_cell(self, rnn_size, keep_probability):\n",
    "        \"\"\"Creates LSTM cell wrapped with dropout.\n",
    "        \"\"\"\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(rnn_size)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_probability)\n",
    "        return cell\n",
    "\n",
    "    def make_attention_cell(self, dec_cell, rnn_size, enc_output, lengths, alignment_history=False):\n",
    "        \"\"\"Wraps the given cell with Bahdanau Attention.\n",
    "        \"\"\"\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size,\n",
    "                                                                   memory=enc_output,\n",
    "                                                                   memory_sequence_length=lengths,\n",
    "                                                                   name='BahdanauAttention')\n",
    "\n",
    "        return tf.contrib.seq2seq.AttentionWrapper(cell=dec_cell,\n",
    "                                                   attention_mechanism=attention_mechanism,\n",
    "                                                   attention_layer_size=None,\n",
    "                                                   output_attention=False,\n",
    "                                                   alignment_history=alignment_history)\n",
    "\n",
    "    def triangular_lr(self, current_step):\n",
    "        \"\"\"cyclic learning rate - exponential range.\"\"\"\n",
    "        step_size = self.learning_rate_decay_steps\n",
    "        base_lr = self.learning_rate\n",
    "        max_lr = self.max_lr\n",
    "\n",
    "        cycle = tf.floor(1 + current_step / (2 * step_size))\n",
    "        x = tf.abs(current_step / step_size - 2 * cycle + 1)\n",
    "        lr = base_lr + (max_lr - base_lr) * tf.maximum(0.0, tf.cast((1.0 - x), dtype=tf.float32)) * (0.99999 ** tf.cast(\n",
    "            current_step,\n",
    "            dtype=tf.float32))\n",
    "        return lr\n",
    "\n",
    "\n",
    "    def add_seq2seq(self):\n",
    "        \"\"\"Creates the sequence to sequence architecture.\"\"\"\n",
    "        with tf.variable_scope('dynamic_seq2seq', dtype=tf.float32):\n",
    "            # Encoder\n",
    "            encoder_outputs, encoder_state = self.build_encoder()\n",
    "\n",
    "            # Decoder\n",
    "            logits, sample_id, final_context_state = self.build_decoder(encoder_outputs,\n",
    "                                                                        encoder_state)\n",
    "            if self.mode == 'TRAIN':\n",
    "\n",
    "                # Loss\n",
    "                loss = self.compute_loss(logits)\n",
    "                self.train_loss = loss\n",
    "                self.eval_loss = loss\n",
    "                self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "\n",
    "                # cyclic learning rate\n",
    "                if self.use_cyclic_lr:\n",
    "                    self.learning_rate = self.triangular_lr(self.global_step)\n",
    "\n",
    "                # exponential learning rate\n",
    "                else:\n",
    "                    self.learning_rate = tf.train.exponential_decay(\n",
    "                        self.learning_rate,\n",
    "                        self.global_step,\n",
    "                        decay_steps=self.learning_rate_decay_steps,\n",
    "                        decay_rate=self.learning_rate_decay,\n",
    "                        staircase=True)\n",
    "\n",
    "                # Optimizer\n",
    "                opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "\n",
    "                # Gradients\n",
    "                if self.clip > 0:\n",
    "                    grads, vs = zip(*opt.compute_gradients(self.train_loss))\n",
    "                    grads, _ = tf.clip_by_global_norm(grads, self.clip)\n",
    "                    self.train_op = opt.apply_gradients(zip(grads, vs),\n",
    "                                                        global_step=self.global_step)\n",
    "                else:\n",
    "                    self.train_op = opt.minimize(self.train_loss,\n",
    "                                                 global_step=self.global_step)\n",
    "\n",
    "\n",
    "\n",
    "            elif self.mode == 'INFER':\n",
    "                loss = None\n",
    "                self.infer_logits, _, self.final_context_state, self.sample_id = logits, loss, final_context_state, sample_id\n",
    "                self.sample_words = self.sample_id\n",
    "\n",
    "    def build_encoder(self):\n",
    "        \"\"\"The encoder. Bidirectional LSTM.\"\"\"\n",
    "\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            fw_cell = self.make_rnn_cell(self.rnn_size_encoder // 2, self.keep_probability)\n",
    "            bw_cell = self.make_rnn_cell(self.rnn_size_encoder // 2, self.keep_probability)\n",
    "\n",
    "            for _ in range(self.num_layers_encoder):\n",
    "                (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw=fw_cell,\n",
    "                    cell_bw=bw_cell,\n",
    "                    inputs=self.word_embeddings_1,\n",
    "                    sequence_length=self.sequence_lengths_1,\n",
    "                    dtype=tf.float32)\n",
    "                encoder_outputs = tf.concat((out_fw, out_bw), -1)\n",
    "\n",
    "            bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n",
    "            bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n",
    "            bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
    "            encoder_state = tuple([bi_lstm_state] * self.num_layers_encoder)\n",
    "\n",
    "            return encoder_outputs, encoder_state\n",
    "\n",
    "\n",
    "    def build_decoder(self, encoder_outputs, encoder_state):\n",
    "\n",
    "        sos_id_2 = tf.cast(self.word2ind[self.sos], tf.int32)\n",
    "        eos_id_2 = tf.cast(self.word2ind[self.eos], tf.int32)\n",
    "        self.output_layer = Dense(self.vocab_size, name='output_projection')\n",
    "\n",
    "        # Decoder.\n",
    "        with tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "\n",
    "            cell, decoder_initial_state = self.build_decoder_cell(\n",
    "                encoder_outputs,\n",
    "                encoder_state,\n",
    "                self.sequence_lengths_1)\n",
    "\n",
    "            # Train\n",
    "            if self.mode != 'INFER':\n",
    "\n",
    "                helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs=self.word_embeddings_2,\n",
    "                    sequence_length=self.sequence_lengths_2,\n",
    "                    embedding=self.embedding,\n",
    "                    sampling_probability=0.5,\n",
    "                    time_major=False)\n",
    "\n",
    "                # Decoder\n",
    "                my_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                             helper,\n",
    "                                                             decoder_initial_state,\n",
    "                                                             output_layer=self.output_layer)\n",
    "\n",
    "                # Dynamic decoding\n",
    "                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    my_decoder,\n",
    "                    output_time_major=False,\n",
    "                    maximum_iterations=self.maximum_iterations,\n",
    "                    swap_memory=False,\n",
    "                    impute_finished=True,\n",
    "                    scope=decoder_scope\n",
    "                )\n",
    "\n",
    "                sample_id = outputs.sample_id\n",
    "                logits = outputs.rnn_output\n",
    "\n",
    "\n",
    "            # Inference\n",
    "            else:\n",
    "                start_tokens = tf.fill([self.batch_size], sos_id_2)\n",
    "                end_token = eos_id_2\n",
    "\n",
    "                # beam search\n",
    "                if self.beam_width > 0:\n",
    "                    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                        cell=cell,\n",
    "                        embedding=self.embedding,\n",
    "                        start_tokens=start_tokens,\n",
    "                        end_token=end_token,\n",
    "                        initial_state=decoder_initial_state,\n",
    "                        beam_width=self.beam_width,\n",
    "                        output_layer=self.output_layer,\n",
    "                    )\n",
    "\n",
    "                # greedy\n",
    "                else:\n",
    "                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embedding,\n",
    "                                                                      start_tokens,\n",
    "                                                                      end_token)\n",
    "\n",
    "                    my_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                                 helper,\n",
    "                                                                 decoder_initial_state,\n",
    "                                                                 output_layer=self.output_layer)\n",
    "                if self.inference_targets:\n",
    "                    maximum_iterations = self.maximum_iterations\n",
    "                else:\n",
    "                    maximum_iterations = None\n",
    "\n",
    "                # Dynamic decoding\n",
    "                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    my_decoder,\n",
    "                    maximum_iterations=maximum_iterations,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=False,\n",
    "                    swap_memory=False,\n",
    "                    scope=decoder_scope)\n",
    "\n",
    "                if self.beam_width > 0:\n",
    "                    logits = tf.no_op()\n",
    "                    sample_id = outputs.predicted_ids\n",
    "                else:\n",
    "                    logits = outputs.rnn_output\n",
    "                    sample_id = outputs.sample_id\n",
    "\n",
    "        return logits, sample_id, final_context_state\n",
    "\n",
    "    def build_decoder_cell(self, encoder_outputs, encoder_state,\n",
    "                           sequence_lengths_1):\n",
    "        \"\"\"Builds the attention decoder cell. If mode is inference performs tiling\n",
    "           Passes last encoder state.\n",
    "        \"\"\"\n",
    "\n",
    "        memory = encoder_outputs\n",
    "\n",
    "        if self.mode == 'INFER' and self.beam_width > 0:\n",
    "            memory = tf.contrib.seq2seq.tile_batch(memory,\n",
    "                                                   multiplier=self.beam_width)\n",
    "            encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n",
    "                                                          multiplier=self.beam_width)\n",
    "            sequence_lengths_1 = tf.contrib.seq2seq.tile_batch(sequence_lengths_1,\n",
    "                                                               multiplier=self.beam_width)\n",
    "            batch_size = self.batch_size * self.beam_width\n",
    "\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        # MY APPROACH\n",
    "        if self.num_layers_decoder is not None:\n",
    "            lstm_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [self.make_rnn_cell(self.rnn_size_decoder, self.keep_probability) for _ in\n",
    "                 range(self.num_layers_decoder)])\n",
    "\n",
    "        else:\n",
    "            lstm_cell = self.make_rnn_cell(self.rnn_size_decoder, self.keep_probability)\n",
    "\n",
    "        # attention cell\n",
    "        cell = self.make_attention_cell(lstm_cell,\n",
    "                                        self.rnn_size_decoder,\n",
    "                                        memory,\n",
    "                                        sequence_lengths_1)\n",
    "\n",
    "        decoder_initial_state = cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "\n",
    "        return cell, decoder_initial_state\n",
    "\n",
    "\n",
    "    def compute_loss(self, logits):\n",
    "        \"\"\"Compute the loss during optimization.\"\"\"\n",
    "        target_output = self.ids_2\n",
    "        max_time = self.maximum_iterations\n",
    "\n",
    "        target_weights = tf.sequence_mask(self.sequence_lengths_2,\n",
    "                                          max_time,\n",
    "                                          dtype=tf.float32,\n",
    "                                          name='mask')\n",
    "\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits=logits,\n",
    "                                                targets=target_output,\n",
    "                                                weights=target_weights,\n",
    "                                                average_across_timesteps=True,\n",
    "                                                average_across_batch=True, )\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self,\n",
    "              inputs,\n",
    "              targets,\n",
    "              restore_path=None,\n",
    "              validation_inputs=None,\n",
    "              validation_targets=None):\n",
    "        \"\"\"Performs the training process. Runs training step in every epoch.\n",
    "           Shuffles input data before every epoch.\n",
    "           Optionally: - add tensorboard summaries.\n",
    "                       - restoring previous model and retraining on top.\n",
    "                       - evaluation step.\n",
    "        \"\"\"\n",
    "        assert len(inputs) == len(targets)\n",
    "\n",
    "        if self.summary_dir is not None:\n",
    "            self.add_summary()\n",
    "\n",
    "        self.initialize_session()\n",
    "        if restore_path is not None:\n",
    "            self.restore_session(restore_path)\n",
    "\n",
    "        best_score = np.inf\n",
    "        nepoch_no_imprv = 0\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        for epoch in range(self.epochs + 1):\n",
    "            print('-------------------- Epoch {} of {} --------------------'.format(epoch,\n",
    "                                                                                    self.epochs))\n",
    "\n",
    "            # shuffle the input data before every epoch.\n",
    "            shuffle_indices = np.random.permutation(len(inputs))\n",
    "            inputs = inputs[shuffle_indices]\n",
    "            targets = targets[shuffle_indices]\n",
    "\n",
    "            # run training epoch\n",
    "            score = self.run_epoch(inputs, targets, epoch)\n",
    "\n",
    "            # evaluate model\n",
    "            if validation_inputs is not None and validation_targets is not None:\n",
    "                self.run_evaluate(validation_inputs, validation_targets, epoch)\n",
    "\n",
    "\n",
    "            #if not os.path.exists(self.save_path):\n",
    "            #        os.makedirs(self.save_path)\n",
    "            #    self.saver.save(self.sess, self.save_path)\n",
    "                \n",
    "            if score <= best_score:\n",
    "                nepoch_no_imprv = 0\n",
    "                if not os.path.exists(self.save_path):\n",
    "                    os.makedirs(self.save_path)\n",
    "                self.saver.save(self.sess, self.save_path)\n",
    "                best_score = score\n",
    "                print(\"--- new best score ---\\n\\n\")\n",
    "            else:\n",
    "                # warm up epochs for the model\n",
    "                if epoch > 10:\n",
    "                    nepoch_no_imprv += 1\n",
    "                # early stopping\n",
    "                if nepoch_no_imprv >= 5:\n",
    "                    print(\"- early stopping {} epochs without improvement\".format(nepoch_no_imprv))\n",
    "                    break\n",
    "\n",
    "    def infer(self, inputs, restore_path, targets=None):\n",
    "        \"\"\"Runs inference process. No training takes place.\n",
    "           Returns the predicted ids for every sentence.\n",
    "        \"\"\"\n",
    "        self.initialize_session()\n",
    "        self.restore_session(restore_path)\n",
    "\n",
    "        prediction_ids = []\n",
    "        if targets is not None:\n",
    "            feed, _, sequence_lengths_2 = self.get_feed_dict(inputs, trgts=targets)\n",
    "        else:\n",
    "            feed, _ = self.get_feed_dict(inputs)\n",
    "\n",
    "        infer_logits, s_ids = self.sess.run([self.infer_logits, self.sample_words], feed_dict=feed)\n",
    "        prediction_ids.append(s_ids)\n",
    "\n",
    "        # for (inps, trgts) in summarizer_model_utils.minibatches(inputs, targets, self.batch_size):\n",
    "        #     feed, _, sequence_lengths= self.get_feed_dict(inps, trgts=trgts)\n",
    "        #     infer_logits, s_ids = self.sess.run([self.infer_logits, self.sample_words], feed_dict = feed)\n",
    "        #     prediction_ids.append(s_ids)\n",
    "\n",
    "        return prediction_ids\n",
    "\n",
    "    def run_epoch(self, inputs, targets, epoch):\n",
    "        \"\"\"Runs a single epoch.\n",
    "           Returns the average loss value on the epoch.\"\"\"\n",
    "        batch_size = self.batch_size\n",
    "        nbatches = (len(inputs) + batch_size - 1) // batch_size\n",
    "        losses = []\n",
    "\n",
    "        for i, (inps, trgts) in enumerate(minibatches(inputs,\n",
    "                                                                             targets,\n",
    "                                                                             batch_size)):\n",
    "            if inps is not None and trgts is not None:\n",
    "                fd, sl, s2 = self.get_feed_dict(inps,\n",
    "                                                trgts=trgts)\n",
    "\n",
    "                if i % 10 == 0 and self.summary_dir is not None:\n",
    "                    _, train_loss, training_summ = self.sess.run([self.train_op,\n",
    "                                                                  self.train_loss,\n",
    "                                                                  self.training_summary],\n",
    "                                                                 feed_dict=fd)\n",
    "                    self.training_writer.add_summary(training_summ, epoch*nbatches + i)\n",
    "\n",
    "                else:\n",
    "                    _, train_loss = self.sess.run([self.train_op, self.train_loss],\n",
    "                                                  feed_dict=fd)\n",
    "\n",
    "                if i % 2 == 0 or i == (nbatches - 1):\n",
    "                    print('Iteration: {} of {}\\ttrain_loss: {:.4f}'.format(i, nbatches - 1, train_loss))\n",
    "                losses.append(train_loss)\n",
    "\n",
    "            else:\n",
    "                print('Minibatch empty.')\n",
    "                continue\n",
    "\n",
    "        avg_loss = self.sess.run(tf.reduce_mean(losses))\n",
    "        print('Average Score for this Epoch: {}'.format(avg_loss))\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def run_evaluate(self, inputs, targets, epoch):\n",
    "        \"\"\"Runs evaluation on validation inputs and targets.\n",
    "        Optionally: - writes summary to Tensorboard.\n",
    "        \"\"\"\n",
    "        if self.summary_dir is not None:\n",
    "            eval_losses = []\n",
    "            for inps, trgts in minibatches(inputs, targets, self.batch_size):\n",
    "                fd, sl, s2 = self.get_feed_dict(inps, trgts)\n",
    "                eval_loss = self.sess.run([self.eval_loss], feed_dict=fd)\n",
    "                eval_losses.append(eval_loss)\n",
    "\n",
    "            avg_eval_loss = self.sess.run(tf.reduce_mean(eval_losses))\n",
    "\n",
    "            print('Eval_loss: {}\\n'.format(avg_eval_loss))\n",
    "            eval_summ = self.sess.run([self.eval_summary], feed_dict=fd)\n",
    "           # self.eval_writer.add_summary(eval_summ, epoch)\n",
    "\n",
    "        else:\n",
    "            eval_losses = []\n",
    "            for inps, trgts in minibatches(inputs, targets, self.batch_size):\n",
    "                fd, sl, s2 = self.get_feed_dict(inps, trgts)\n",
    "                eval_loss = self.sess.run([self.eval_loss], feed_dict=fd)\n",
    "                eval_losses.append(eval_loss)\n",
    "\n",
    "            avg_eval_loss = self.sess.run(tf.reduce_mean(eval_losses))\n",
    "\n",
    "            print('Eval_loss: {}\\n'.format(avg_eval_loss))\n",
    "\n",
    "\n",
    "\n",
    "    def get_feed_dict(self, inps, trgts=None):\n",
    "        \"\"\"Creates the feed_dict that is fed into training or inference network.\n",
    "           Pads inputs and targets.\n",
    "           Returns feed_dict and sequence_length(s) depending on training mode.\n",
    "        \"\"\"\n",
    "        if self.mode != 'INFER':\n",
    "            inp_ids, sequence_lengths_1 = pad_sequences(inps,\n",
    "                                                                               self.word2ind[self.pad],\n",
    "                                                                               tail=False)\n",
    "\n",
    "            feed = {\n",
    "                self.ids_1: inp_ids,\n",
    "                self.sequence_lengths_1: sequence_lengths_1\n",
    "            }\n",
    "\n",
    "            if trgts is not None:\n",
    "                trgt_ids, sequence_lengths_2 = pad_sequences(trgts,\n",
    "                                                                                    self.word2ind[self.pad],\n",
    "                                                                                    tail=True)\n",
    "                feed[self.ids_2] = trgt_ids\n",
    "                feed[self.sequence_lengths_2] = sequence_lengths_2\n",
    "\n",
    "                return feed, sequence_lengths_1, sequence_lengths_2\n",
    "\n",
    "        else:\n",
    "\n",
    "            inp_ids, sequence_lengths_1 = pad_sequences(inps,\n",
    "                                                                               self.word2ind[self.pad],\n",
    "                                                                               tail=False)\n",
    "\n",
    "            feed = {\n",
    "                self.ids_1: inp_ids,\n",
    "                self.sequence_lengths_1: sequence_lengths_1\n",
    "            }\n",
    "\n",
    "            if trgts is not None:\n",
    "                trgt_ids, sequence_lengths_2 = pad_sequences(trgts,\n",
    "                                                                                    self.word2ind[self.pad],\n",
    "                                                                                    tail=True)\n",
    "\n",
    "                feed[self.sequence_lengths_2] = sequence_lengths_2\n",
    "\n",
    "                return feed, sequence_lengths_1, sequence_lengths_2\n",
    "            else:\n",
    "                return feed, sequence_lengths_1\n",
    "\n",
    "    def initialize_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def restore_session(self, restore_path):\n",
    "        self.saver.restore(self.sess, restore_path)\n",
    "        print('Done.')\n",
    "\n",
    "    def add_summary(self):\n",
    "        \"\"\"Summaries for Tensorboard.\"\"\"\n",
    "        self.training_summary = tf.summary.scalar('training_loss', self.train_loss)\n",
    "        self.eval_summary = tf.summary.scalar('evaluation_loss', self.eval_loss)\n",
    "        self.training_writer = tf.summary.FileWriter(self.summary_dir,\n",
    "                                                     tf.get_default_graph())\n",
    "        self.eval_writer = tf.summary.FileWriter(self.summary_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 3.3: INITIALIZE THE SEQ2SEQ MODEL\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph and train the model \n",
    "reset_graph()\n",
    "summarizer = Summarizer(word2ind,\n",
    "                        ind2word,\n",
    "                        save_path='drive/Colab Notebooks/Model 3/my_model',\n",
    "                        mode='TRAIN',\n",
    "                        num_layers_encoder = num_layers_encoder,\n",
    "                        num_layers_decoder = num_layers_decoder,\n",
    "                        rnn_size_encoder = rnn_size_encoder,\n",
    "                        rnn_size_decoder = rnn_size_decoder,\n",
    "                        batch_size = batch_size,\n",
    "                        clip = clip,\n",
    "                        keep_probability = keep_probability,\n",
    "                        learning_rate = learning_rate,\n",
    "                        max_lr=max_lr,\n",
    "                        learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                        learning_rate_decay = learning_rate_decay,\n",
    "                        epochs = epochs,\n",
    "                        pretrained_embeddings_path = pretrained_embeddings_path,\n",
    "                        use_cyclic_lr = use_cyclic_lr,\n",
    "                        summary_dir = summary_dir)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 3.4: TRAIN THE SEQ2SEQ MODEL\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summarizer.build_graph()\n",
    "summarizer.train(converted_texts[:70976], \n",
    "                 converted_summaries[:70976],\n",
    "                 validation_inputs=converted_texts[70976:],\n",
    "                 validation_targets=converted_summaries[70976:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Inference\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 4.1: INITIALIZE THE SEQ2SEQ MODEL FOR INFERENCE\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Summarizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-43cbdc527547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m summarizer = Summarizer(word2ind,\n\u001b[0m\u001b[1;32m      3\u001b[0m                         \u001b[0mind2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0;34m'saved_model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0;34m'INFER'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Summarizer' is not defined"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "summarizer = Summarizer(word2ind,\n",
    "                        ind2word,\n",
    "                        'saved_model/',\n",
    "                        'INFER',\n",
    "                        num_layers_encoder = num_layers_encoder,\n",
    "                        num_layers_decoder = num_layers_decoder,\n",
    "                        batch_size = len(converted_texts[:50]),\n",
    "                        clip = clip,\n",
    "                        keep_probability = 1.0,\n",
    "                        learning_rate = 0.0,\n",
    "                        beam_width = 5,\n",
    "                        rnn_size_encoder = rnn_size_encoder,\n",
    "                        rnn_size_decoder = rnn_size_decoder,\n",
    "                        inference_targets = True,\n",
    "                        pretrained_embeddings_path = pretrained_embeddings_path)\n",
    "\n",
    "summarizer.build_graph()\n",
    "preds = summarizer.infer(converted_texts[:50],\n",
    "                         restore_path = 'saved_model/',\n",
    "                         targets = converted_summaries[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 4.2: PREDICT THE VALIDATION SET\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_results(preds, ind2word, word2ind, converted_summaries, converted_texts, use_bleu=False):\n",
    "    \"\"\"\n",
    "    Plots the actual text and summary and the corresponding created summary.\n",
    "    takes care of whether beam search or greedy decoder was used.\n",
    "    \"\"\"\n",
    "    beam = False\n",
    "\n",
    "    if len(np.array(preds).shape) == 4:\n",
    "        beam = True\n",
    "\n",
    "    '''Bleu score is not used correctly here, but serves as reference.\n",
    "    '''\n",
    "    if use_bleu:\n",
    "        bleu_scores = []\n",
    "\n",
    "    for pred, summary, text, seq_length in zip(preds[0],\n",
    "                                               converted_summaries,\n",
    "                                               converted_texts,\n",
    "                                               [len(inds) for inds in converted_summaries]):\n",
    "        print('\\n\\n\\n', 100 * '-')\n",
    "        if beam:\n",
    "            actual_text = [ind2word[word] for word in text if\n",
    "                           word != word2ind[\"<SOS>\"] and word != word2ind[\"<EOS>\"]]\n",
    "            actual_summary = [ind2word[word] for word in summary if\n",
    "                              word != word2ind['<EOS>'] and word != word2ind['<SOS>']]\n",
    "\n",
    "            created_summary = []\n",
    "            for word in pred:\n",
    "                if word[0] != word2ind['<SOS>'] and word[0] != word2ind['<EOS>']:\n",
    "                    created_summary.append(ind2word[word[0]])\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            print('Actual Text:\\n{}\\n'.format(' '.join(actual_text)))\n",
    "            print('Actual Summary:\\n{}\\n'.format(' '.join(actual_summary)))\n",
    "            print('Created Summary:\\n{}\\n'.format(' '.join(created_summary)))\n",
    "            if use_bleu:\n",
    "                bleu_score = sentence_bleu([actual_summary], created_summary)\n",
    "                bleu_scores.append(bleu_score)\n",
    "                print('Bleu-score:', bleu_score)\n",
    "\n",
    "            print()\n",
    "\n",
    "\n",
    "        else:\n",
    "            actual_text = [ind2word[word] for word in text if\n",
    "                           word != word2ind[\"<SOS>\"] and word != word2ind[\"<EOS>\"]]\n",
    "            actual_summary = [ind2word[word] for word in summary if\n",
    "                              word != word2ind['<EOS>'] and word != word2ind['<SOS>']]\n",
    "            created_summary = [ind2word[word] for word in pred if\n",
    "                               word != word2ind['<EOS>'] and word != word2ind['<SOS>']]\n",
    "\n",
    "            print('Actual Text:\\n{}\\n'.format(' '.join(actual_text)))\n",
    "            print('Actual Summary:\\n{}\\n'.format(' '.join(actual_summary)))\n",
    "            print('Created Summary:\\n{}\\n'.format(' '.join(created_summary)))\n",
    "            if use_bleu:\n",
    "                bleu_score = sentence_bleu([actual_summary], created_summary)\n",
    "                bleu_scores.append(bleu_score)\n",
    "                print('Bleu-score:', bleu_score)\n",
    "\n",
    "    if use_bleu:\n",
    "        bleu_score = np.mean(bleu_scores)\n",
    "        print('\\n\\n\\nTotal Bleu Score:', bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "sample_results(preds,\n",
    "               ind2word,\n",
    "               word2ind,\n",
    "               converted_summaries[:50],\n",
    "               converted_texts[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 4.3: CALCULATE THE ACCURACY\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 4.4: PREDICT UN-SEEN SENTENCES\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THE END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Improvements\n",
    "\n",
    "---\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Resources\n",
    "\n",
    "---\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. \n",
    "\n",
    "1. <a>Github Blog Series Text Summarization</a> Thanks to Blah. A lot of ideas got from his codes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
