{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "<hr>\n",
    "\n",
    "***Automatic Text Summarization*** is the process of shortening a text document using the deep learning methods. More specifically we will use a seq2seq model for this purpose. This model is widely used in industry today. Search engines are an example. Others include summarization of documents, image collections and videos. \n",
    "\n",
    "<img width=\"600px\" src=\"assets/intro.png\">\n",
    "<br><br>\n",
    "In this Jupyter Notebook, we will go through the following chapters:\n",
    "\n",
    "- **Chapter 1:** Data Prepration\n",
    "- **Chapter 2:** Data Preprocessing & Embedding\n",
    "- **Chapter 3:** Training the Seq2Seq Model\n",
    "- **Chapter 4:** Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "# Chapter 1: Data Prepration\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this chapter, we will make our data ready. Furthermore, we make sure the necessary libraries have been installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 1.1: DOWNLOAD AND IMPORT PACKAGES\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now, let's import all the packages and libraries that we are going to use throughout this jupyter notebook. Also make sure that you have installed Tensorflow version 1.12 otherwise the codes won't run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install contractions\n",
    "!pip install tensorflow==1.12\n",
    "!pip install tensorflow_hub\n",
    "!pip install nltk\n",
    "!pip install textsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import time\n",
    "import re\n",
    "import html\n",
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import contractions\n",
    "\n",
    "# Import the deep learning libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.python.layers.core import Dense\n",
    "#import seq2seq\n",
    "\n",
    "# Import the NLP libraries\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download punkt sentence tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  1.12.0\n"
     ]
    }
   ],
   "source": [
    "# Check the tensorflow version\n",
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "assert tf.__version__ == \"1.12.0\", print(\"Please Install Tensorflow Version 1.12.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 1.2: DOWNLOAD & LOAD THE DATASET\n",
    "\n",
    "<hr>\n",
    "\n",
    "The dataset that we are going to use is called \"Amazon Find Food Reviews\". This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories. Our aim is to input a review (Text column) and automatically create a summary (Summary colum) for it. \n",
    "\n",
    "<img width=\"400px\" src=\"assets/amazon_food.png\">\n",
    "\n",
    "You can download the dataset from the following link:\n",
    "\n",
    "https://www.kaggle.com/snap/amazon-fine-food-reviews/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4894,
     "status": "ok",
     "timestamp": 1526227108183,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "2OiSxpApmw0o",
    "outputId": "98a255ad-248e-4f1e-b43f-1c8d384fd453"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rElWMbT2mw0t",
    "outputId": "08f4b9ee-8c78-4f3b-c986-f7f3dd2ff248"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at first 5 rows of dataset\n",
    "data.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:  (568454, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 1.3: EXPLORE THE DATASET\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this part, we will make our dataset to be ready for data preprocessing. More specifically, we will remove the missing values, we will get the relevant columns and ignore the rest, and lastly we will remove the texts if it's too large or if it's too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1055,
     "status": "ok",
     "timestamp": 1526227113630,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "N9bHztjpmw0x",
    "outputId": "aa4ce93d-efde-4ebb-bc3e-7b03d477a5e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: \n",
      "\n",
      " Id                         0\n",
      "ProductId                  0\n",
      "UserId                     0\n",
      "ProfileName               16\n",
      "HelpfulnessNumerator       0\n",
      "HelpfulnessDenominator     0\n",
      "Score                      0\n",
      "Time                       0\n",
      "Summary                   27\n",
      "Text                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the total missing values\n",
    "print(\"Missing Values: \\n\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "boMCgsgTmw00"
   },
   "outputs": [],
   "source": [
    "# Drop the missing values in \"Summary\" column\n",
    "data.dropna(subset = [\"Summary\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1526227125421,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ESv4XLgQmw03",
    "outputId": "0ca5d3f7-7efc-46dc-bf8e-bb5803c6376c"
   },
   "outputs": [],
   "source": [
    "# Take only the columns \"Summary\" and \"Text\"\n",
    "data = data[['Summary', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the dataset\n",
    "data.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BjmIGbXtmw08"
   },
   "outputs": [],
   "source": [
    "### Get the text with length between 100 and 150\n",
    "\n",
    "# Initialize the empty lists\n",
    "texts = []\n",
    "summaries = []\n",
    "\n",
    "# Iterate through text and its summary\n",
    "for i_text, i_summary in zip(data[\"Text\"], data[\"Summary\"]):\n",
    "    \n",
    "    # If length is between 100 and 150\n",
    "    if 100 < len(i_text) < 150:\n",
    "        \n",
    "        # Append the text and summary into list\n",
    "        texts.append(i_text)\n",
    "        summaries.append(i_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1560,
     "status": "ok",
     "timestamp": 1526227148045,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "t5JBoyqKmw0_",
    "outputId": "150af52c-a0af-4eec-f0ff-399ec33e434d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Texts Lists:  78862\n",
      "Length of Summaries Lists:  78862\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Texts Lists: \", len(texts))\n",
    "print(\"Length of Summaries Lists: \", len(summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tMsDeec4mw1F",
    "outputId": "de46147e-d2c4-40e5-9a0a-22f27ac360fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "Summary:  Great taffy\n",
      "\n",
      "\n",
      "Text:  This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!\n",
      "Summary:  Wonderful, tasty taffy\n",
      "\n",
      "\n",
      "Text:  Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too\n",
      "Summary:  Yay Barley\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first 3 texts and its summary\n",
    "for i_text, i_summary in zip(texts[:3], summaries[:3]):\n",
    "    print(\"Text: \", i_text)\n",
    "    print(\"Summary: \", i_summary)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Data Preprocessing & Embedding\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this chapter we will apply all the necessary preprocessing steps into our text. Then we will create embedding for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 2.1: CLEAN AND PREPARE THE DATASET\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this lessson, we will write down some function for creating mini-batches and sequence padding. Afterward we will apply the text preprocessing steps for making our dataset ready for neural network. More specifically, we will apply the following steps:\n",
    "1. Lowercase the text\n",
    "2. Fix the contractions (e.g. she's -> she is, he's -> he is, etc.)\n",
    "3. Remove the punctuations\n",
    "4. Remove some specific characters inside the given dataset\n",
    "5. Remove the extra spaces\n",
    "6. Tokenizing the texts into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_steps(text, keep_most = False):\n",
    "    \"\"\"\n",
    "    Function for preprocessing the sentenes.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ===================\n",
    "        - text: The text that we want to perform the preprocessing.\n",
    "        - keep_most: Depending if True or False, we either keep only letters and numbers or also other \n",
    "                     characters.\n",
    "        \n",
    "    RETURNS\n",
    "    ===================\n",
    "        - text: Preprocessed text.\n",
    "    \"\"\"\n",
    "    # Lower case the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove and replace some specific words\n",
    "    text = fixup(text)\n",
    "    \n",
    "    # Fix the contractions (e.g. she's -> she is, he's -> he is, etc.)\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove the punctuation \n",
    "    if keep_most:\n",
    "        text = re.sub(r\"[^a-z0-9%!?.,:()/]\", \" \", text)\n",
    "    else:\n",
    "        text = re.sub(r\"[^a-z0-9]\", \" \", text)\n",
    "    \n",
    "    # Remove the breaks\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "        \n",
    "    # Remove the extra spaces\n",
    "    text = re.sub(r\"    \", \" \", text)\n",
    "    text = re.sub(r\"   \", \" \", text)\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixup(x):\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(text, keep_most = False):\n",
    "    \"\"\"\n",
    "    Split the text intp sentences. Then preprocess them. Afterward tokenize the sentence.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ===================\n",
    "        - text: The text that we want to perform the preprocessing.\n",
    "        - keep_most: depending if True or False, we either keep only letters and numbers or also other characters.\n",
    "        \n",
    "    RETURNS\n",
    "    ===================\n",
    "        - tokenized: Tokenized & preprocessed text.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list\n",
    "    tokenized = []\n",
    "    \n",
    "    # Iterate through tokenized sentences\n",
    "    for i_sentence in nltk.sent_tokenize(text):\n",
    "        \n",
    "        # Apply the preprocessing_steps function\n",
    "        i_sentence = preprocessing_steps(i_sentence, keep_most)\n",
    "        \n",
    "        # Tokenize into words\n",
    "        i_sentence = nltk.word_tokenize(i_sentence)\n",
    "        \n",
    "        # Iterate through tokenized words\n",
    "        for token in i_sentence:\n",
    "            \n",
    "            # Append the tokens into the list\n",
    "            tokenized.append(token)\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(texts, summaries, keep_most = False): # preprocess_texts_and_summaries\n",
    "    \"\"\"\n",
    "    Apply all the preprocessing steps to our texts and summaries.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ===================\n",
    "        - texts: The texts that we want to perform the preprocessing.\n",
    "        - summaries: The summaries that we want to perform the preprocessing.\n",
    "        - keep_most: depending if True or False, we either keep only letters and numbers or also other characters.\n",
    "        \n",
    "    RETURNS\n",
    "    ===================\n",
    "        - processed texts\n",
    "        - processed summaries\n",
    "        - words_counted: Array containing all the unique words together with their counts sorted by counts.\n",
    "    \"\"\"\n",
    "    # Initialize the lists\n",
    "    processed_texts = []\n",
    "    processed_summaries = []\n",
    "    words = []\n",
    "\n",
    "    # Iterate through texts\n",
    "    for i_text in tqdm.tqdm(texts):\n",
    "        \n",
    "        # Preprocess the text\n",
    "        i_text = preprocess_and_tokenize(i_text, keep_most)\n",
    "        \n",
    "        # Iterate through each words\n",
    "        for i_word in i_text:\n",
    "            \n",
    "            # Append the word into words list\n",
    "            words.append(i_word)\n",
    "            \n",
    "        # Append the preprocessed text into processed_texts\n",
    "        processed_texts.append(i_text)\n",
    "        \n",
    "    # Iterate through summaries\n",
    "    for i_summary in tqdm.tqdm(summaries):\n",
    "        \n",
    "        # Preprocess the summary\n",
    "        i_summary = preprocess_and_tokenize(i_summary, keep_most)\n",
    "        \n",
    "        # Iterate through each words\n",
    "        for i_word in i_summary:\n",
    "            \n",
    "            # Append the word into words\n",
    "            words.append(i_word)\n",
    "\n",
    "        # Append the preprocessed summary into processed_summaries\n",
    "        processed_summaries.append(i_summary)\n",
    "        \n",
    "    # Create word count\n",
    "    words_counted = Counter(words).most_common()\n",
    "\n",
    "    return processed_texts, processed_summaries, words_counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78862/78862 [01:34<00:00, 832.90it/s] \n",
      "100%|██████████| 78862/78862 [00:23<00:00, 3427.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the texts and summaries\n",
    "processed_texts, processed_summaries, words_counted = apply_preprocessing(texts, \n",
    "                                                                          summaries, \n",
    "                                                                          keep_most = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yquphHYJmw1R",
    "outputId": "cd9ad917-33da-4294-eb41-f3b0abb967e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text:  ['great', 'taffy', 'at', 'a', 'great', 'price', 'there', 'was', 'a', 'wide', 'assortment', 'of', 'yummy', 'taffy', 'delivery', 'was', 'very', 'quick', 'if', 'your', 'a', 'taffy', 'lover', 'this', 'is', 'a', 'deal']\n",
      "Preprocessed Summary:  ['great', 'taffy']\n",
      "\n",
      "\n",
      "Preprocessed Text:  ['this', 'taffy', 'is', 'so', 'good', 'it', 'is', 'very', 'soft', 'and', 'chewy', 'the', 'flavors', 'are', 'amazing', 'i', 'would', 'definitely', 'recommend', 'you', 'buying', 'it', 'very', 'satisfying']\n",
      "Preprocessed Summary:  ['wonderful', 'tasty', 'taffy']\n",
      "\n",
      "\n",
      "Preprocessed Text:  ['right', 'now', 'i', 'm', 'mostly', 'just', 'sprouting', 'this', 'so', 'my', 'cats', 'can', 'eat', 'the', 'grass', 'they', 'love', 'it', 'i', 'rotate', 'it', 'around', 'with', 'wheatgrass', 'and', 'rye', 'too']\n",
      "Preprocessed Summary:  ['yay', 'barley']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the preprocessed texts and summaries\n",
    "for i_text, i_summary in zip(processed_texts[:3], processed_summaries[:3]):\n",
    "    print(\"Preprocessed Text: \", i_text)\n",
    "    print(\"Preprocessed Summary: \", i_summary)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 2.2: CREATE LOOKUP DICTIONARIES\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now it's time to create two dictionaries; One that converts text into integers (word2int) and another one that converts integers into text (int2word).\n",
    "\n",
    "<img width=\"800px\" src=\"assets/int2word.jpeg\">\n",
    "<p style=\"font-size:9px; text-align:right; color:gray;\" >Image Taken From hackernoon.com</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_inds_dicts(words_counted, specials = None, min_occurences = 0):\n",
    "    \"\"\" \n",
    "    Create lookup dicts from word to index and back. \n",
    "\n",
    "    ARGUMENTS\n",
    "    ===================\n",
    "        - words_counted\n",
    "        - specials\n",
    "        - min_occurences\n",
    "        \n",
    "    RETURNS\n",
    "    ===================\n",
    "        - word2ind\n",
    "        - ind2word\n",
    "        - missing_words\n",
    "    \"\"\"\n",
    "    # Initialize empty list\n",
    "    missing_words = []\n",
    "    \n",
    "    # Initialize empty dictionary\n",
    "    word2ind = {}\n",
    "    ind2word = {}\n",
    "    \n",
    "    # Initialize the index\n",
    "    index = 0\n",
    "\n",
    "    # If specials has been specified\n",
    "    if specials is not None:\n",
    "        \n",
    "        # Iterate through specials\n",
    "        for i_special in specials:\n",
    "            \n",
    "            # Append the special and its index into word2ind\n",
    "            word2ind[i_special] = index\n",
    "            \n",
    "            # Append the special and its index into ind2word\n",
    "            ind2word[index] = i_special\n",
    "            \n",
    "            # Increment the index\n",
    "            index += 1\n",
    "\n",
    "    # Iterate through word counts\n",
    "    for (i_word, i_count) in words_counted:\n",
    "        \n",
    "        # If count is greater than or equal to min_occurences\n",
    "        if i_count >= min_occurences:\n",
    "            \n",
    "            # Append the word and its index into word2ind\n",
    "            word2ind[i_word] = index\n",
    "            \n",
    "            # Append the word and its index into ind2word\n",
    "            ind2word[index] = i_word\n",
    "            \n",
    "            # Increment the index\n",
    "            index += 1\n",
    "        \n",
    "        # If count is less than or equal to min_occurences\n",
    "        else:\n",
    "            \n",
    "            # Append the word into missing_words\n",
    "            missing_words.append(i_word)\n",
    "\n",
    "    return word2ind, ind2word, missing_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(review, word2ind):\n",
    "    \"\"\" \n",
    "    Convert the given sentence to integer values corresponding to the given word2ind.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ===================\n",
    "        - review: The given text that we want to convert to integer.\n",
    "        - word2ind: A dictionary that maps each word into an integer number.\n",
    "        \n",
    "    RETURNS\n",
    "    ===================\n",
    "        - integers: Integers corresponding ot the given sentence.\n",
    "        - unknown_words: Word that were not in word2ind. \n",
    "    \"\"\"\n",
    "    # Initialize empty lists\n",
    "    integers = []\n",
    "    unknown_words = []\n",
    "\n",
    "    # Iterate through each word in review\n",
    "    for i_word in review:\n",
    "        \n",
    "        # If word is in word2ind\n",
    "        if i_word in word2ind.keys():\n",
    "            \n",
    "            # Append integer corresponding to the word\n",
    "            integers.append(int(word2ind[i_word]))\n",
    "            \n",
    "        # If word is NOT in word2ind\n",
    "        else:\n",
    "            \n",
    "            # Append integer corresponding '<UNK>' \n",
    "            integers.append(int(word2ind['<UNK>']))\n",
    "            \n",
    "            # Append the word into unknown_words\n",
    "            unknown_words.append(word)\n",
    "\n",
    "    return integers, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_inds(input, word2ind, eos = False, sos = False):\n",
    "    \"\"\"\n",
    "    Convert the integers.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ===================\n",
    "        - input: The input\n",
    "        - word2ind: A dictionary that maps each word into an integer number\n",
    "        - eos: End of sentence\n",
    "        - sos: Start of sentence\n",
    "        \n",
    "    RETURNS\n",
    "    ===================\n",
    "        - converted_input\n",
    "        - all_unknown_words\n",
    "    \"\"\"\n",
    "    # Initialize an empty list\n",
    "    converted_input = []\n",
    "    \n",
    "    # Initialize a set\n",
    "    all_unknown_words = set()\n",
    "\n",
    "    # Iterate through inputs\n",
    "    for i_input in input:\n",
    "        \n",
    "        # Convert the given sentence into integers\n",
    "        converted_inp, unknown_words = convert_sentence(i_input, word2ind)\n",
    "        \n",
    "        # If end of sentence\n",
    "        if eos:\n",
    "            \n",
    "            # Append the integer correponding to <EOS> into converted_inp\n",
    "            converted_inp.append(word2ind['<EOS>'])\n",
    "            \n",
    "        # If start of sentence\n",
    "        if sos:\n",
    "            \n",
    "            # Append the integer correponding to <SOS> into converted_inp\n",
    "            converted_inp.insert(0, word2ind['<SOS>'])\n",
    "            \n",
    "        # Append converted_inp into converted_input\n",
    "        converted_input.append(converted_inp)\n",
    "        \n",
    "        # Append unknown_words into all_unknown_words\n",
    "        all_unknown_words.update(unknown_words)\n",
    "\n",
    "    return converted_input, all_unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_inds_to_text(inds, ind2word):\n",
    "    \"\"\" \n",
    "    Convert the given indexes back to text \n",
    "    \n",
    "    ARGUMENTS\n",
    "    ===================\n",
    "        - inds: Integers we want to convert to text.\n",
    "        - ind2word: A dictionary that maps integers into words.\n",
    "        \n",
    "    RETURNS\n",
    "    ===================\n",
    "        - words: Coverted words\n",
    "    \"\"\"\n",
    "    # Iterate through integers and get the words using ind2word dictionary\n",
    "    words = [ind2word[word] for word in inds]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of word2ind:  25031\n",
      "Length of ind2word:  25031\n",
      "Length of Missing Words:  0\n"
     ]
    }
   ],
   "source": [
    "# Specials\n",
    "specials = [\"<EOS>\", \"<SOS>\", \"<PAD>\", \"<UNK>\"]\n",
    "\n",
    "# Create lookup dicts from word to index and back\n",
    "word2ind, ind2word,  missing_words = create_word_inds_dicts(words_counted, specials = specials)\n",
    "\n",
    "# Print the lengths\n",
    "print(\"Length of word2ind: \", len(word2ind))\n",
    "print(\"Length of ind2word: \", len(ind2word))\n",
    "print(\"Length of Missing Words: \", len(missing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert texts into integers (looks like we have to set eos here to False)\n",
    "converted_texts, unknown_words_in_texts = convert_to_inds(processed_texts, \n",
    "                                                          word2ind, \n",
    "                                                          eos = False)\n",
    "\n",
    "# Covert summaries into integers\n",
    "converted_summaries, unknown_words_in_summaries = convert_to_inds(processed_summaries, \n",
    "                                                                  word2ind, \n",
    "                                                                  eos = True, \n",
    "                                                                  sos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer text: \n",
      " [12, 1707, 45, 8, 12, 44, 130, 29, 8, 2701, 1144, 16, 104, 1707, 317, 29, 26, 247, 61, 99, 8, 1707, 647, 10, 9, 8, 192] \n",
      "\n",
      "Integer summary: \n",
      " [1, 12, 1707, 0] \n",
      "\n",
      "Converted Text: \n",
      " ['great', 'taffy', 'at', 'a', 'great', 'price', 'there', 'was', 'a', 'wide', 'assortment', 'of', 'yummy', 'taffy', 'delivery', 'was', 'very', 'quick', 'if', 'your', 'a', 'taffy', 'lover', 'this', 'is', 'a', 'deal'] \n",
      "\n",
      "Converted Summary: \n",
      " ['<SOS>', 'great', 'taffy', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(\"Integer text: \\n\", converted_texts[0], \"\\n\")\n",
    "print(\"Integer summary: \\n\", converted_summaries[0], \"\\n\")\n",
    "print(\"Converted Text: \\n\", convert_inds_to_text(converted_texts[0], ind2word), \"\\n\")\n",
    "print(\"Converted Summary: \\n\", convert_inds_to_text(converted_summaries[0], ind2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 2.3: EMBEDDING  \n",
    "\n",
    "<hr>\n",
    "\n",
    "Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. Word embeddings are distributed representations of text in an n-dimensional space. These are essential for solving most NLP problems. Optionally we can use pretrained word embeddings. Those have proved to increase training speed and accuracy. Here we can use two different options: glove embedding and tf_hub embedding. However the ones from tf_hub worked better.\n",
    "\n",
    "<img width=\"300px\" src=\"assets/embeddings.png\">\n",
    "<p style=\"font-size:9px; text-align:right; color:gray;\" >Image Taken From udacity.com</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embeddings(path):\n",
    "    \"\"\"\n",
    "    Loading the pretrained embeddings which it stores each embedding in a dictionary with its corresponding word.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary\n",
    "    embeddings = {}\n",
    "    \n",
    "    # Open the file\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        \n",
    "        # Iterate through each line\n",
    "        for line in f:\n",
    "            \n",
    "            # Split the line\n",
    "            values = line.split(' ')\n",
    "            \n",
    "            # Get the word\n",
    "            word = values[0]\n",
    "            \n",
    "            # Get the embedding vector\n",
    "            embedding_vector = np.array(values[1:], dtype='float32')\n",
    "            \n",
    "            # Append the embedding vector into embeddings dictionary\n",
    "            embeddings[word] = embedding_vector\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_embedding_matrix(word2ind, pretrained_embeddings_path, save_path, embedding_dim = 300):\n",
    "    \"\"\"\n",
    "    Creating an embedding matrix for each word in word2ind. if that words is in pretrained_embeddings, \n",
    "    that vector is used. otherwise initialized randomly.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ===================\n",
    "        - word2ind: A dictionary that maps each word into an integer number.\n",
    "        - pretrained_embeddings_path: Path to the pre-trained embedding.\n",
    "        - save_path: Path for saving the embedding matrix.\n",
    "        - embedding_dim: Embedding dimension.\n",
    "        \n",
    "    RETURNS\n",
    "    ===================\n",
    "        - embedding_matrix: Embedding matrix\n",
    "        \n",
    "    \"\"\"\n",
    "    # Load the pre-trained embeddings\n",
    "    pretrained_embeddings = load_pretrained_embeddings(pretrained_embeddings_path)\n",
    "    \n",
    "    # Initialize the embedding matrix with zeros\n",
    "    embedding_matrix = np.zeros((len(word2ind), embedding_dim), dtype=np.float32)\n",
    "    \n",
    "    # Iterate through each word and its index\n",
    "    for i_word, i_index in word2ind.items():\n",
    "        \n",
    "        # If word is inside the pre-trained embeddings key\n",
    "        if i_word in pretrained_embeddings.keys():\n",
    "            \n",
    "            # Append into embedding_matrix \n",
    "            embedding_matrix[i_index] = pretrained_embeddings[i_word]\n",
    "            \n",
    "        # If word is NOT inside the pre-trained embeddings key\n",
    "        else:\n",
    "            \n",
    "            # Initialize a uniformly random embedding\n",
    "            embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            \n",
    "            # Append the embedding into embedding_matrix\n",
    "            embedding_matrix[i_index] = embedding\n",
    "            \n",
    "    # If save_path does not exist\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        \n",
    "        # Make save_path directory\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "        \n",
    "    # Save the embedding matrix\n",
    "    np.save(save_path, embedding_matrix)\n",
    "    \n",
    "    return np.array(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to glove embedding\n",
    "#glove_embeddings_path = './glove/glove.6B.300d.txt'\n",
    "\n",
    "# Path to save the embedding matrix\n",
    "#embedding_matrix_save_path = './embeddings/embedding.npy'\n",
    "\n",
    "# Create an embedding matrix for each word in word2ind\n",
    "#emb = create_and_save_embedding_matrix(word2ind, glove_embeddings_path, embedding_matrix_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings from tf_hub.\n",
    "embed = hub.Module(\"https://tfhub.dev/google/Wiki-words-250/1\")      # embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "emb = embed([key for key in word2ind.keys()]) \n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Initialize all tables of the default graph\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    # Run the session\n",
    "    embedding = sess.run(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1526227413774,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ayXi9D7Umw1u",
    "outputId": "7b5b5522-8c21-4c70-a5be-46f6ed2cdbd2"
   },
   "outputs": [],
   "source": [
    "print(\"Embedding Size: \", embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QoGa9EWdmw11"
   },
   "outputs": [],
   "source": [
    "# Save the embedding\n",
    "np.save('embeddings/embedding.npy', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Chapter 3: Training the Seq2Seq Model\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this chapter, we will start training our seq2seq model for text summarization. We will begin with setting our hyperparameters, then we will initialize our model and afterward train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 3.1: SET HYPERPARAMETERS & PATHS\n",
    "\n",
    "<hr>\n",
    "\n",
    "Before initializing the seq2seq model, let's set our hyperparameters. We will come back to this part more often in order to change the hyperparameters. This is a important part because it determines the big part of our accuracy. So take your time for changing the value and monitoring the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tEItjpP4mw2J"
   },
   "outputs": [],
   "source": [
    "# model hyperparametes\n",
    "num_layers_encoder = 4\n",
    "num_layers_decoder = 4\n",
    "rnn_size_encoder = 512\n",
    "rnn_size_decoder = 512\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 15\n",
    "clip = 5\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.0005\n",
    "max_lr=0.005\n",
    "learning_rate_decay_steps = 700\n",
    "learning_rate_decay = 0.90\n",
    "\n",
    "\n",
    "pretrained_embeddings_path = 'embeddings/embedding.npy'\n",
    "summary_dir = os.path.join('./tensorboard', str('Nn_' + str(rnn_size_encoder) + '_Lr_' + str(learning_rate)))\n",
    "\n",
    "\n",
    "use_cyclic_lr = True\n",
    "inference_targets=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1464,
     "status": "ok",
     "timestamp": 1526234914336,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "u8lJ_OI5mw2Q",
    "outputId": "1c06bc51-01eb-4a68-b4ca-38d56a4a2a76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78862"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1526234915582,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "w_VDuiHyQK84",
    "outputId": "9bc17a2e-837b-41bd-a40d-0f116e143d8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70976"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(78862*0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 3.2: IMPLEMENT THE SEQ2SEQ MODEL\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now let's implement the seq2seq model. Every seq2seq model is divided into two parts: encoder and decoder. The encoder encodes the input sequence into a fixed-length context vector. This vector is an internal representation of the text. This context vector is then decoded into the output sequence by the decoder. \n",
    "\n",
    "<img src=\"assets/encoder-decoder.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed = 97):\n",
    "    \"\"\"\n",
    "    Function for reseting the default graph.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Clear the default graph stack and reset the global default graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Set the random seed in tensorflow\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    # Set the random seed in numpy\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(inputs, targets, minibatch_size):\n",
    "    \"\"\"batch generator. yields x and y batch.\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    for inp, tgt in zip(inputs, targets):\n",
    "        if len(x_batch) == minibatch_size and len(y_batch) == minibatch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "        x_batch.append(inp)\n",
    "        y_batch.append(tgt)\n",
    "\n",
    "    if len(x_batch) != 0:\n",
    "        for inp, tgt in zip(inputs, targets):\n",
    "            if len(x_batch) != minibatch_size:\n",
    "                x_batch.append(inp)\n",
    "                y_batch.append(tgt)\n",
    "            else:\n",
    "                break\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_tok, tail=True):\n",
    "    \"\"\"Pads the sentences, so that all sentences in a batch have the same length.\n",
    "    \"\"\"\n",
    "\n",
    "    max_length = max(len(x) for x in sequences)\n",
    "\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        if tail:\n",
    "            seq_ = seq[:max_length] + [pad_tok] * max(max_length - len(seq), 0)\n",
    "        else:\n",
    "            seq_ = [pad_tok] * max(max_length - len(seq), 0) + seq[:max_length]\n",
    "\n",
    "        sequence_padded += [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq:\n",
    "\n",
    "    def __init__(self,\n",
    "                 word2ind,\n",
    "                 ind2word,\n",
    "                 save_path,\n",
    "                 mode='TRAIN',\n",
    "                 num_layers_encoder=1,\n",
    "                 num_layers_decoder=1,\n",
    "                 embedding_dim=300,\n",
    "                 rnn_size_encoder=256,\n",
    "                 rnn_size_decoder=256,\n",
    "                 learning_rate=0.001,\n",
    "                 learning_rate_decay=0.9,\n",
    "                 learning_rate_decay_steps=100,\n",
    "                 max_lr=0.01,\n",
    "                 keep_probability=0.8,\n",
    "                 batch_size=64,\n",
    "                 beam_width=10,\n",
    "                 epochs=20,\n",
    "                 eos=\"<EOS>\",\n",
    "                 sos=\"<SOS>\",\n",
    "                 pad='<PAD>',\n",
    "                 clip=5,\n",
    "                 inference_targets=False,\n",
    "                 pretrained_embeddings_path=None,\n",
    "                 summary_dir=None,\n",
    "                 use_cyclic_lr=False):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            word2ind: lookup dict from word to index.\n",
    "            ind2word: lookup dict from index to word.\n",
    "            save_path: path to save the tf model to in the end.\n",
    "            mode: String. 'TRAIN' or 'INFER'. depending on which mode we use\n",
    "                  a different graph is created.\n",
    "            num_layers_encoder: Float. Number of encoder layers. defaults to 1.\n",
    "            num_layers_decoder: Float. Number of decoder layers. defaults to 1.\n",
    "            embedding_dim: dimension of the embedding vectors in the embedding matrix.\n",
    "                           every word has a embedding_dim 'long' vector.\n",
    "            rnn_size_encoder: Integer. number of hidden units in encoder. defaults to 256.\n",
    "            rnn_size_decoder: Integer. number of hidden units in decoder. defaults to 256.\n",
    "            learning_rate: Float.\n",
    "            learning_rate_decay: only if exponential learning rate is used.\n",
    "            learning_rate_decay_steps: Integer.\n",
    "            max_lr: only used if cyclic learning rate is used.\n",
    "            keep_probability: Float.\n",
    "            batch_size: Integer. Size of minibatches.\n",
    "            beam_width: Integer. Only used in inference, for Beam Search.('INFER'-mode)\n",
    "            epochs: Integer. Number of times the training is conducted\n",
    "                    on the whole training data.\n",
    "            eos: EndOfSentence tag.\n",
    "            sos: StartOfSentence tag.\n",
    "            pad: Padding tag.\n",
    "            clip: Value to clip the gradients to in training process.\n",
    "            inference_targets:\n",
    "            pretrained_embeddings_path: Path to pretrained embeddings. Has to be .npy\n",
    "            summary_dir: Directory the summaries are written to for tensorboard.\n",
    "            use_cyclic_lr: Boolean.\n",
    "        \"\"\"\n",
    "\n",
    "        self.word2ind = word2ind\n",
    "        self.ind2word = ind2word\n",
    "        self.vocab_size = len(word2ind)\n",
    "        self.num_layers_encoder = num_layers_encoder\n",
    "        self.num_layers_decoder = num_layers_decoder\n",
    "        self.rnn_size_encoder = rnn_size_encoder\n",
    "        self.rnn_size_decoder = rnn_size_decoder\n",
    "        self.save_path = save_path\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mode = mode.upper()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.learning_rate_decay_steps = learning_rate_decay_steps\n",
    "        self.keep_probability = keep_probability\n",
    "        self.batch_size = batch_size\n",
    "        self.beam_width = beam_width\n",
    "        self.eos = eos\n",
    "        self.sos = sos\n",
    "        self.clip = clip\n",
    "        self.pad = pad\n",
    "        self.epochs = epochs\n",
    "        self.inference_targets = inference_targets\n",
    "        self.pretrained_embeddings_path = pretrained_embeddings_path\n",
    "        self.use_cyclic_lr = use_cyclic_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.summary_dir = summary_dir\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.add_placeholders()\n",
    "        self.add_embeddings()\n",
    "        self.add_lookup_ops()\n",
    "        self.initialize_session()\n",
    "        self.add_seq2seq()\n",
    "        self.saver = tf.train.Saver()\n",
    "        print('Graph built.')\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.ids_1 = tf.placeholder(tf.int32,\n",
    "                                    shape=[None, None],\n",
    "                                    name='ids_source')\n",
    "        self.ids_2 = tf.placeholder(tf.int32,\n",
    "                                    shape=[None, None],\n",
    "                                    name='ids_target')\n",
    "        self.sequence_lengths_1 = tf.placeholder(tf.int32,\n",
    "                                                 shape=[None],\n",
    "                                                 name='sequence_length_source')\n",
    "        self.sequence_lengths_2 = tf.placeholder(tf.int32,\n",
    "                                                 shape=[None],\n",
    "                                                 name='sequence_length_target')\n",
    "        self.maximum_iterations = tf.reduce_max(self.sequence_lengths_2,\n",
    "                                                name='max_dec_len')\n",
    "\n",
    "    def create_word_embedding(self, embed_name, vocab_size, embed_dim):\n",
    "        \"\"\"Creates embedding matrix in given shape - [vocab_size, embed_dim].\n",
    "        \"\"\"\n",
    "        embedding = tf.get_variable(embed_name,\n",
    "                                    shape=[vocab_size, embed_dim],\n",
    "                                    dtype=tf.float32)\n",
    "        return embedding\n",
    "\n",
    "    def add_embeddings(self):\n",
    "        \"\"\"Creates the embedding matrix. In case path to pretrained embeddings is given,\n",
    "           that embedding is loaded. Otherwise created.\n",
    "        \"\"\"\n",
    "        if self.pretrained_embeddings_path is not None:\n",
    "            self.embedding = tf.Variable(np.load(self.pretrained_embeddings_path),\n",
    "                                         name='embedding')\n",
    "            print('Loaded pretrained embeddings.')\n",
    "        else:\n",
    "            self.embedding = self.create_word_embedding('embedding',\n",
    "                                                        self.vocab_size,\n",
    "                                                        self.embedding_dim)\n",
    "\n",
    "    def add_lookup_ops(self):\n",
    "        \"\"\"Additional lookup operation for both source embedding and target embedding matrix.\n",
    "        \"\"\"\n",
    "        self.word_embeddings_1 = tf.nn.embedding_lookup(self.embedding,\n",
    "                                                        self.ids_1,\n",
    "                                                        name='word_embeddings_1')\n",
    "        self.word_embeddings_2 = tf.nn.embedding_lookup(self.embedding,\n",
    "                                                        self.ids_2,\n",
    "                                                        name='word_embeddings_2')\n",
    "\n",
    "    def make_rnn_cell(self, rnn_size, keep_probability):\n",
    "        \"\"\"Creates LSTM cell wrapped with dropout.\n",
    "        \"\"\"\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(rnn_size)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_probability)\n",
    "        return cell\n",
    "\n",
    "    def make_attention_cell(self, dec_cell, rnn_size, enc_output, lengths, alignment_history=False):\n",
    "        \"\"\"Wraps the given cell with Bahdanau Attention.\n",
    "        \"\"\"\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size,\n",
    "                                                                   memory=enc_output,\n",
    "                                                                   memory_sequence_length=lengths,\n",
    "                                                                   name='BahdanauAttention')\n",
    "\n",
    "        return tf.contrib.seq2seq.AttentionWrapper(cell=dec_cell,\n",
    "                                                   attention_mechanism=attention_mechanism,\n",
    "                                                   attention_layer_size=None,\n",
    "                                                   output_attention=False,\n",
    "                                                   alignment_history=alignment_history)\n",
    "\n",
    "    def triangular_lr(self, current_step):\n",
    "        \"\"\"cyclic learning rate - exponential range.\"\"\"\n",
    "        step_size = self.learning_rate_decay_steps\n",
    "        base_lr = self.learning_rate\n",
    "        max_lr = self.max_lr\n",
    "\n",
    "        cycle = tf.floor(1 + current_step / (2 * step_size))\n",
    "        x = tf.abs(current_step / step_size - 2 * cycle + 1)\n",
    "        lr = base_lr + (max_lr - base_lr) * tf.maximum(0.0, tf.cast((1.0 - x), dtype=tf.float32)) * (0.99999 ** tf.cast(\n",
    "            current_step,\n",
    "            dtype=tf.float32))\n",
    "        return lr\n",
    "\n",
    "\n",
    "    def add_seq2seq(self):\n",
    "        \"\"\"Creates the sequence to sequence architecture.\"\"\"\n",
    "        with tf.variable_scope('dynamic_seq2seq', dtype=tf.float32):\n",
    "            # Encoder\n",
    "            encoder_outputs, encoder_state = self.build_encoder()\n",
    "\n",
    "            # Decoder\n",
    "            logits, sample_id, final_context_state = self.build_decoder(encoder_outputs,\n",
    "                                                                        encoder_state)\n",
    "            if self.mode == 'TRAIN':\n",
    "\n",
    "                # Loss\n",
    "                loss = self.compute_loss(logits)\n",
    "                self.train_loss = loss\n",
    "                self.eval_loss = loss\n",
    "                self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "\n",
    "                # cyclic learning rate\n",
    "                if self.use_cyclic_lr:\n",
    "                    self.learning_rate = self.triangular_lr(self.global_step)\n",
    "\n",
    "                # exponential learning rate\n",
    "                else:\n",
    "                    self.learning_rate = tf.train.exponential_decay(\n",
    "                        self.learning_rate,\n",
    "                        self.global_step,\n",
    "                        decay_steps=self.learning_rate_decay_steps,\n",
    "                        decay_rate=self.learning_rate_decay,\n",
    "                        staircase=True)\n",
    "\n",
    "                # Optimizer\n",
    "                opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "\n",
    "                # Gradients\n",
    "                if self.clip > 0:\n",
    "                    grads, vs = zip(*opt.compute_gradients(self.train_loss))\n",
    "                    grads, _ = tf.clip_by_global_norm(grads, self.clip)\n",
    "                    self.train_op = opt.apply_gradients(zip(grads, vs),\n",
    "                                                        global_step=self.global_step)\n",
    "                else:\n",
    "                    self.train_op = opt.minimize(self.train_loss,\n",
    "                                                 global_step=self.global_step)\n",
    "\n",
    "\n",
    "\n",
    "            elif self.mode == 'INFER':\n",
    "                loss = None\n",
    "                self.infer_logits, _, self.final_context_state, self.sample_id = logits, loss, final_context_state, sample_id\n",
    "                self.sample_words = self.sample_id\n",
    "\n",
    "    def build_encoder(self):\n",
    "        \"\"\"The encoder. Bidirectional LSTM.\"\"\"\n",
    "\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            fw_cell = self.make_rnn_cell(self.rnn_size_encoder // 2, self.keep_probability)\n",
    "            bw_cell = self.make_rnn_cell(self.rnn_size_encoder // 2, self.keep_probability)\n",
    "\n",
    "            for _ in range(self.num_layers_encoder):\n",
    "                (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw=fw_cell,\n",
    "                    cell_bw=bw_cell,\n",
    "                    inputs=self.word_embeddings_1,\n",
    "                    sequence_length=self.sequence_lengths_1,\n",
    "                    dtype=tf.float32)\n",
    "                encoder_outputs = tf.concat((out_fw, out_bw), -1)\n",
    "\n",
    "            bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n",
    "            bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n",
    "            bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
    "            encoder_state = tuple([bi_lstm_state] * self.num_layers_encoder)\n",
    "\n",
    "            return encoder_outputs, encoder_state\n",
    "\n",
    "\n",
    "    def build_decoder(self, encoder_outputs, encoder_state):\n",
    "\n",
    "        sos_id_2 = tf.cast(self.word2ind[self.sos], tf.int32)\n",
    "        eos_id_2 = tf.cast(self.word2ind[self.eos], tf.int32)\n",
    "        self.output_layer = Dense(self.vocab_size, name='output_projection')\n",
    "\n",
    "        # Decoder.\n",
    "        with tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "\n",
    "            cell, decoder_initial_state = self.build_decoder_cell(\n",
    "                encoder_outputs,\n",
    "                encoder_state,\n",
    "                self.sequence_lengths_1)\n",
    "\n",
    "            # Train\n",
    "            if self.mode != 'INFER':\n",
    "\n",
    "                helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs=self.word_embeddings_2,\n",
    "                    sequence_length=self.sequence_lengths_2,\n",
    "                    embedding=self.embedding,\n",
    "                    sampling_probability=0.5,\n",
    "                    time_major=False)\n",
    "\n",
    "                # Decoder\n",
    "                my_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                             helper,\n",
    "                                                             decoder_initial_state,\n",
    "                                                             output_layer=self.output_layer)\n",
    "\n",
    "                # Dynamic decoding\n",
    "                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    my_decoder,\n",
    "                    output_time_major=False,\n",
    "                    maximum_iterations=self.maximum_iterations,\n",
    "                    swap_memory=False,\n",
    "                    impute_finished=True,\n",
    "                    scope=decoder_scope\n",
    "                )\n",
    "\n",
    "                sample_id = outputs.sample_id\n",
    "                logits = outputs.rnn_output\n",
    "\n",
    "\n",
    "            # Inference\n",
    "            else:\n",
    "                start_tokens = tf.fill([self.batch_size], sos_id_2)\n",
    "                end_token = eos_id_2\n",
    "\n",
    "                # beam search\n",
    "                if self.beam_width > 0:\n",
    "                    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                        cell=cell,\n",
    "                        embedding=self.embedding,\n",
    "                        start_tokens=start_tokens,\n",
    "                        end_token=end_token,\n",
    "                        initial_state=decoder_initial_state,\n",
    "                        beam_width=self.beam_width,\n",
    "                        output_layer=self.output_layer,\n",
    "                    )\n",
    "\n",
    "                # greedy\n",
    "                else:\n",
    "                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embedding,\n",
    "                                                                      start_tokens,\n",
    "                                                                      end_token)\n",
    "\n",
    "                    my_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                                 helper,\n",
    "                                                                 decoder_initial_state,\n",
    "                                                                 output_layer=self.output_layer)\n",
    "                if self.inference_targets:\n",
    "                    maximum_iterations = self.maximum_iterations\n",
    "                else:\n",
    "                    maximum_iterations = None\n",
    "\n",
    "                # Dynamic decoding\n",
    "                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    my_decoder,\n",
    "                    maximum_iterations=maximum_iterations,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=False,\n",
    "                    swap_memory=False,\n",
    "                    scope=decoder_scope)\n",
    "\n",
    "                if self.beam_width > 0:\n",
    "                    logits = tf.no_op()\n",
    "                    sample_id = outputs.predicted_ids\n",
    "                else:\n",
    "                    logits = outputs.rnn_output\n",
    "                    sample_id = outputs.sample_id\n",
    "\n",
    "        return logits, sample_id, final_context_state\n",
    "\n",
    "    def build_decoder_cell(self, encoder_outputs, encoder_state,\n",
    "                           sequence_lengths_1):\n",
    "        \"\"\"Builds the attention decoder cell. If mode is inference performs tiling\n",
    "           Passes last encoder state.\n",
    "        \"\"\"\n",
    "\n",
    "        memory = encoder_outputs\n",
    "\n",
    "        if self.mode == 'INFER' and self.beam_width > 0:\n",
    "            memory = tf.contrib.seq2seq.tile_batch(memory,\n",
    "                                                   multiplier=self.beam_width)\n",
    "            encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n",
    "                                                          multiplier=self.beam_width)\n",
    "            sequence_lengths_1 = tf.contrib.seq2seq.tile_batch(sequence_lengths_1,\n",
    "                                                               multiplier=self.beam_width)\n",
    "            batch_size = self.batch_size * self.beam_width\n",
    "\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        # MY APPROACH\n",
    "        if self.num_layers_decoder is not None:\n",
    "            lstm_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [self.make_rnn_cell(self.rnn_size_decoder, self.keep_probability) for _ in\n",
    "                 range(self.num_layers_decoder)])\n",
    "\n",
    "        else:\n",
    "            lstm_cell = self.make_rnn_cell(self.rnn_size_decoder, self.keep_probability)\n",
    "\n",
    "        # attention cell\n",
    "        cell = self.make_attention_cell(lstm_cell,\n",
    "                                        self.rnn_size_decoder,\n",
    "                                        memory,\n",
    "                                        sequence_lengths_1)\n",
    "\n",
    "        decoder_initial_state = cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "\n",
    "        return cell, decoder_initial_state\n",
    "\n",
    "\n",
    "    def compute_loss(self, logits):\n",
    "        \"\"\"Compute the loss during optimization.\"\"\"\n",
    "        target_output = self.ids_2\n",
    "        max_time = self.maximum_iterations\n",
    "\n",
    "        target_weights = tf.sequence_mask(self.sequence_lengths_2,\n",
    "                                          max_time,\n",
    "                                          dtype=tf.float32,\n",
    "                                          name='mask')\n",
    "\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits=logits,\n",
    "                                                targets=target_output,\n",
    "                                                weights=target_weights,\n",
    "                                                average_across_timesteps=True,\n",
    "                                                average_across_batch=True, )\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self,\n",
    "              inputs,\n",
    "              targets,\n",
    "              restore_path=None,\n",
    "              validation_inputs=None,\n",
    "              validation_targets=None):\n",
    "        \"\"\"Performs the training process. Runs training step in every epoch.\n",
    "           Shuffles input data before every epoch.\n",
    "           Optionally: - add tensorboard summaries.\n",
    "                       - restoring previous model and retraining on top.\n",
    "                       - evaluation step.\n",
    "        \"\"\"\n",
    "        assert len(inputs) == len(targets)\n",
    "\n",
    "        if self.summary_dir is not None:\n",
    "            self.add_summary()\n",
    "\n",
    "        self.initialize_session()\n",
    "        if restore_path is not None:\n",
    "            self.restore_session(restore_path)\n",
    "\n",
    "        best_score = np.inf\n",
    "        nepoch_no_imprv = 0\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        for epoch in range(self.epochs + 1):\n",
    "            print(\"============> STARTING EPOCH {}/{} <============\".format(epoch, self.epochs))\n",
    "\n",
    "            # shuffle the input data before every epoch.\n",
    "            shuffle_indices = np.random.permutation(len(inputs))\n",
    "            inputs = inputs[shuffle_indices]\n",
    "            targets = targets[shuffle_indices]\n",
    "\n",
    "            # run training epoch\n",
    "            score = self.run_epoch(inputs, targets, epoch)\n",
    "\n",
    "            # evaluate model\n",
    "            if validation_inputs is not None and validation_targets is not None:\n",
    "                self.run_evaluate(validation_inputs, validation_targets, epoch)\n",
    "\n",
    "\n",
    "            if score <= best_score:\n",
    "                nepoch_no_imprv = 0\n",
    "                if not os.path.exists(self.save_path):\n",
    "                    os.makedirs(self.save_path)\n",
    "                self.saver.save(self.sess, self.save_path)\n",
    "                best_score = score\n",
    "                print(\"--- new best score ---\\n\\n\")\n",
    "            else:\n",
    "                # warm up epochs for the model\n",
    "                if epoch > 10:\n",
    "                    nepoch_no_imprv += 1\n",
    "                # early stopping\n",
    "                if nepoch_no_imprv >= 5:\n",
    "                    print(\"- early stopping {} epochs without improvement\".format(nepoch_no_imprv))\n",
    "                    break\n",
    "\n",
    "    def infer(self, inputs, restore_path, targets=None):\n",
    "        \"\"\"Runs inference process. No training takes place.\n",
    "           Returns the predicted ids for every sentence.\n",
    "        \"\"\"\n",
    "        self.initialize_session()\n",
    "        self.restore_session(restore_path)\n",
    "\n",
    "        prediction_ids = []\n",
    "        if targets is not None:\n",
    "            feed, _, sequence_lengths_2 = self.get_feed_dict(inputs, trgts=targets)\n",
    "        else:\n",
    "            feed, _ = self.get_feed_dict(inputs)\n",
    "\n",
    "        infer_logits, s_ids = self.sess.run([self.infer_logits, self.sample_words], feed_dict=feed)\n",
    "        prediction_ids.append(s_ids)\n",
    "\n",
    "        # for (inps, trgts) in summarizer_model_utils.minibatches(inputs, targets, self.batch_size):\n",
    "        #     feed, _, sequence_lengths= self.get_feed_dict(inps, trgts=trgts)\n",
    "        #     infer_logits, s_ids = self.sess.run([self.infer_logits, self.sample_words], feed_dict = feed)\n",
    "        #     prediction_ids.append(s_ids)\n",
    "\n",
    "        return prediction_ids\n",
    "\n",
    "    def run_epoch(self, inputs, targets, epoch):\n",
    "        \"\"\"Runs a single epoch.\n",
    "           Returns the average loss value on the epoch.\"\"\"\n",
    "        batch_size = self.batch_size\n",
    "        nbatches = (len(inputs) + batch_size - 1) // batch_size\n",
    "        losses = []\n",
    "\n",
    "        for i, (inps, trgts) in enumerate(minibatches(inputs,\n",
    "                                                                             targets,\n",
    "                                                                             batch_size)):\n",
    "            if inps is not None and trgts is not None:\n",
    "                fd, sl, s2 = self.get_feed_dict(inps,\n",
    "                                                trgts=trgts)\n",
    "\n",
    "                if i % 10 == 0 and self.summary_dir is not None:\n",
    "                    _, train_loss, training_summ = self.sess.run([self.train_op,\n",
    "                                                                  self.train_loss,\n",
    "                                                                  self.training_summary],\n",
    "                                                                 feed_dict=fd)\n",
    "                    self.training_writer.add_summary(training_summ, epoch*nbatches + i)\n",
    "\n",
    "                else:\n",
    "                    _, train_loss = self.sess.run([self.train_op, self.train_loss],\n",
    "                                                  feed_dict=fd)\n",
    "\n",
    "                if i % 2 == 0 or i == (nbatches - 1):\n",
    "                    print('Epoch {}/{}... Iteration: {}/{}... Training Loss: {:.4f}'.format(epoch, self.epochs, i, nbatches - 1, train_loss))\n",
    "                losses.append(train_loss)\n",
    "\n",
    "            else:\n",
    "                print('Minibatch empty.')\n",
    "                continue\n",
    "\n",
    "        avg_loss = self.sess.run(tf.reduce_mean(losses))\n",
    "        print('Average Score for this Epoch: {}'.format(avg_loss))\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def run_evaluate(self, inputs, targets, epoch):\n",
    "        \"\"\"Runs evaluation on validation inputs and targets.\n",
    "        Optionally: - writes summary to Tensorboard.\n",
    "        \"\"\"\n",
    "        if self.summary_dir is not None:\n",
    "            eval_losses = []\n",
    "            for inps, trgts in minibatches(inputs, targets, self.batch_size):\n",
    "                fd, sl, s2 = self.get_feed_dict(inps, trgts)\n",
    "                eval_loss = self.sess.run([self.eval_loss], feed_dict=fd)\n",
    "                eval_losses.append(eval_loss)\n",
    "\n",
    "            avg_eval_loss = self.sess.run(tf.reduce_mean(eval_losses))\n",
    "\n",
    "            print('Eval_loss: {}\\n'.format(avg_eval_loss))\n",
    "            eval_summ = self.sess.run([self.eval_summary], feed_dict=fd)\n",
    "            self.eval_writer.add_summary(eval_summ, epoch)\n",
    "\n",
    "        else:\n",
    "            eval_losses = []\n",
    "            for inps, trgts in minibatches(inputs, targets, self.batch_size):\n",
    "                fd, sl, s2 = self.get_feed_dict(inps, trgts)\n",
    "                eval_loss = self.sess.run([self.eval_loss], feed_dict=fd)\n",
    "                eval_losses.append(eval_loss)\n",
    "\n",
    "            avg_eval_loss = self.sess.run(tf.reduce_mean(eval_losses))\n",
    "\n",
    "            print('Eval_loss: {}\\n'.format(avg_eval_loss))\n",
    "\n",
    "\n",
    "\n",
    "    def get_feed_dict(self, inps, trgts=None):\n",
    "        \"\"\"Creates the feed_dict that is fed into training or inference network.\n",
    "           Pads inputs and targets.\n",
    "           Returns feed_dict and sequence_length(s) depending on training mode.\n",
    "        \"\"\"\n",
    "        if self.mode != 'INFER':\n",
    "            inp_ids, sequence_lengths_1 = pad_sequences(inps,\n",
    "                                                                               self.word2ind[self.pad],\n",
    "                                                                               tail=False)\n",
    "\n",
    "            feed = {\n",
    "                self.ids_1: inp_ids,\n",
    "                self.sequence_lengths_1: sequence_lengths_1\n",
    "            }\n",
    "\n",
    "            if trgts is not None:\n",
    "                trgt_ids, sequence_lengths_2 = pad_sequences(trgts,\n",
    "                                                                                    self.word2ind[self.pad],\n",
    "                                                                                    tail=True)\n",
    "                feed[self.ids_2] = trgt_ids\n",
    "                feed[self.sequence_lengths_2] = sequence_lengths_2\n",
    "\n",
    "                return feed, sequence_lengths_1, sequence_lengths_2\n",
    "\n",
    "        else:\n",
    "\n",
    "            inp_ids, sequence_lengths_1 = pad_sequences(inps,\n",
    "                                                                               self.word2ind[self.pad],\n",
    "                                                                               tail=False)\n",
    "\n",
    "            feed = {\n",
    "                self.ids_1: inp_ids,\n",
    "                self.sequence_lengths_1: sequence_lengths_1\n",
    "            }\n",
    "\n",
    "            if trgts is not None:\n",
    "                trgt_ids, sequence_lengths_2 = pad_sequences(trgts,\n",
    "                                                                                    self.word2ind[self.pad],\n",
    "                                                                                    tail=True)\n",
    "\n",
    "                feed[self.sequence_lengths_2] = sequence_lengths_2\n",
    "\n",
    "                return feed, sequence_lengths_1, sequence_lengths_2\n",
    "            else:\n",
    "                return feed, sequence_lengths_1\n",
    "\n",
    "    def initialize_session(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def restore_session(self, restore_path):\n",
    "        self.saver.restore(self.sess, restore_path)\n",
    "        print('Done.')\n",
    "\n",
    "    def add_summary(self):\n",
    "        \"\"\"Summaries for Tensorboard.\"\"\"\n",
    "        self.training_summary = tf.summary.scalar('training_loss', self.train_loss)\n",
    "        self.eval_summary = tf.summary.scalar('evaluation_loss', self.eval_loss)\n",
    "        self.training_writer = tf.summary.FileWriter(self.summary_dir,\n",
    "                                                     tf.get_default_graph())\n",
    "        self.eval_writer = tf.summary.FileWriter(self.summary_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 3.3: INITIALIZE THE SEQ2SEQ MODEL\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph and train the model \n",
    "reset_graph()\n",
    "summarizer = seq2seq(word2ind,\n",
    "                         ind2word,\n",
    "                         save_path = 'saved model',\n",
    "                         mode = 'TRAIN',\n",
    "                         num_layers_encoder = num_layers_encoder,\n",
    "                         num_layers_decoder = num_layers_decoder,\n",
    "                         rnn_size_encoder = rnn_size_encoder,\n",
    "                         rnn_size_decoder = rnn_size_decoder,\n",
    "                         batch_size = batch_size,\n",
    "                         clip = clip,\n",
    "                         keep_probability = keep_probability,\n",
    "                         learning_rate = learning_rate,\n",
    "                         max_lr = max_lr,\n",
    "                         learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                         learning_rate_decay = learning_rate_decay,\n",
    "                         epochs = epochs,\n",
    "                         pretrained_embeddings_path = pretrained_embeddings_path,\n",
    "                         use_cyclic_lr = use_cyclic_lr,\n",
    "                         summary_dir = summary_dir)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 3.4: TRAIN THE SEQ2SEQ MODEL\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer.build_graph()\n",
    "\n",
    "summarizer.train(converted_texts[:70976], \n",
    "                 converted_summaries[:70976],\n",
    "                 validation_inputs = converted_texts[70976:],\n",
    "                 validation_targets = converted_summaries[70976:])\n",
    "\n",
    "# hidden training output.\n",
    "# both train and validation loss decrease nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Inference\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 4.1: INITIALIZE THE SEQ2SEQ MODEL FOR INFERENCE\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4607,
     "status": "ok",
     "timestamp": 1526243454761,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "ljN9a1hemw2Y",
    "outputId": "f60102af-44f0-4c45-8ba3-2548e5af0a4c"
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "summarizer = seq2seq.seq2seq(word2ind,\n",
    "                                   ind2word,\n",
    "                                   'saved model',\n",
    "                                   'INFER',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   batch_size = len(converted_texts[:50]),\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = 1.0,\n",
    "                                   learning_rate = 0.0,\n",
    "                                   beam_width = 5,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   inference_targets = True,\n",
    "                                   pretrained_embeddings_path = pretrained_embeddings_path)\n",
    "\n",
    "summarizer.build_graph()\n",
    "preds = summarizer.infer(converted_texts[:50],\n",
    "                         restore_path =  'saved model',\n",
    "                         targets = converted_summaries[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 4.2: PREDICT THE VALIDATION SET\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_results(preds, ind2word, word2ind, converted_summaries, converted_texts, use_bleu=False):\n",
    "    \"\"\"Plots the actual text and summary and the corresponding created summary.\n",
    "    takes care of whether beam search or greedy decoder was used.\n",
    "    \"\"\"\n",
    "    beam = False\n",
    "\n",
    "    if len(np.array(preds).shape) == 4:\n",
    "        beam = True\n",
    "\n",
    "    '''Bleu score is not used correctly here, but serves as reference.\n",
    "    '''\n",
    "    if use_bleu:\n",
    "        bleu_scores = []\n",
    "\n",
    "    for pred, summary, text, seq_length in zip(preds[0],\n",
    "                                               converted_summaries,\n",
    "                                               converted_texts,\n",
    "                                               [len(inds) for inds in converted_summaries]):\n",
    "        print('\\n\\n\\n', 100 * '-')\n",
    "        if beam:\n",
    "            actual_text = [ind2word[word] for word in text if\n",
    "                           word != word2ind[\"<SOS>\"] and word != word2ind[\"<EOS>\"]]\n",
    "            actual_summary = [ind2word[word] for word in summary if\n",
    "                              word != word2ind['<EOS>'] and word != word2ind['<SOS>']]\n",
    "\n",
    "            created_summary = []\n",
    "            for word in pred:\n",
    "                if word[0] != word2ind['<SOS>'] and word[0] != word2ind['<EOS>']:\n",
    "                    created_summary.append(ind2word[word[0]])\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            print('Actual Text:\\n{}\\n'.format(' '.join(actual_text)))\n",
    "            print('Actual Summary:\\n{}\\n'.format(' '.join(actual_summary)))\n",
    "            print('Created Summary:\\n{}\\n'.format(' '.join(created_summary)))\n",
    "            if use_bleu:\n",
    "                bleu_score = sentence_bleu([actual_summary], created_summary)\n",
    "                bleu_scores.append(bleu_score)\n",
    "                print('Bleu-score:', bleu_score)\n",
    "\n",
    "            print()\n",
    "\n",
    "\n",
    "        else:\n",
    "            actual_text = [ind2word[word] for word in text if\n",
    "                           word != word2ind[\"<SOS>\"] and word != word2ind[\"<EOS>\"]]\n",
    "            actual_summary = [ind2word[word] for word in summary if\n",
    "                              word != word2ind['<EOS>'] and word != word2ind['<SOS>']]\n",
    "            created_summary = [ind2word[word] for word in pred if\n",
    "                               word != word2ind['<EOS>'] and word != word2ind['<SOS>']]\n",
    "\n",
    "            print('Actual Text:\\n{}\\n'.format(' '.join(actual_text)))\n",
    "            print('Actual Summary:\\n{}\\n'.format(' '.join(actual_summary)))\n",
    "            print('Created Summary:\\n{}\\n'.format(' '.join(created_summary)))\n",
    "            if use_bleu:\n",
    "                bleu_score = sentence_bleu([actual_summary], created_summary)\n",
    "                bleu_scores.append(bleu_score)\n",
    "                print('Bleu-score:', bleu_score)\n",
    "\n",
    "    if use_bleu:\n",
    "        bleu_score = np.mean(bleu_scores)\n",
    "        print('\\n\\n\\nTotal Bleu Score:', bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 11917
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1526243456128,
     "user": {
      "displayName": "Thomas Schmied",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "102636220151368904258"
     },
     "user_tz": -120
    },
    "id": "JtB2kNIWmw2j",
    "outputId": "b2b34d18-062a-4ea0-e48f-29ed6c3fe123",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show results\n",
    "sample_results(preds,\n",
    "              ind2word,\n",
    "              word2ind,\n",
    "              converted_summaries[:50],\n",
    "              converted_texts[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 4.3: CALCULATE THE ACCURACY\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LESSION 4.4: PREDICT UN-SEEN SENTENCES\n",
    "\n",
    "<hr>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THE END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo.\n",
    "\n",
    "<br>\n",
    "\n",
    "# Improvements\n",
    "\n",
    "---\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. Nullam vitae ultrices velit. Donec auctor mollis consequat. Vivamus efficitur id ligula nec rhoncus. Sed elit nulla, tincidunt at nunc cursus, facilisis sagittis nunc. Ut varius congue enim, ac ultricies nunc ultrices nec. In hac habitasse platea dictumst. Maecenas urna neque, sodales ac vehicula a, tristique quis nisi. Vestibulum congue rhoncus enim a semper. Aliquam laoreet venenatis nisi sed posuere. Etiam et auctor lacus. Suspendisse ut iaculis turpis, et porttitor risus. Phasellus fringilla purus in faucibus mollis. Fusce facilisis elit in metus consectetur facilisis. In in risus eget risus porta pulvinar iaculis ac quam. Sed eget aliquet est, vel tempor leo.\n",
    "\n",
    "<br>\n",
    "\n",
    "# Resources\n",
    "\n",
    "---\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In sollicitudin mauris quis ante tempus, et sodales leo aliquam. \n",
    "\n",
    "1. <a>Github Blog Series Text Summarization</a> Thanks to Blah. A lot of ideas got from his codes."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "summarizer_amazon_reviews.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
